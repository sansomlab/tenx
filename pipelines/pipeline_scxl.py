
#
#   Kennedy Institute of Rheumatology
#
#   $Id$
#
#   Copyright (C) 2018 Stephen Sansom
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################

"""============
Pipeline scxl
===============

:Author: Sansom lab
:Release: $Id$
:Date: |today|
:Tags: Python

Overview
========

A pipeline for highthroughput analysis of scRNA-seq datasets.

The pipeline has been used to analyse datasets with up to 800,000 cells.

This pipeline relies on both the R Seurat library and Python Scanpy package,
and makes use of many excellent tools from the community including Scran,
DropletUtils, SingleR, Clustree, Destiny (for diffusion maps), PHATE, PAGA
and Scvelo) for downstream analysis. Automatic export of UCSC cell browser
instances is also supported.

For geneset over representation analysis the pipelines use a bespoke R
package called gsfisher (http://github/sansomlab/gsfisher), which can
also be used interactively to analyse single-cell data.

The pipelines are in active development, and should be considered "beta"
software - please use at your own risk!

For key parameters a range of choices can be specified. The pipeline will
generate one report for each parameter combination, dispatching analyses
from multiple samples in parallel for execution on a HPC cluster.


Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_scxl.py config


Input files
-----------

The pipeline can be run either:
(A) starting from a suitable count matrix and metadata file (such as
the output of pipeline_cellranger.py) or
(B) starting from a saved seurat object. This is useful for analysing
an object to which reduced dimensions from another method (e.g. zinbwave)
have been added.
Optionally, velocity plots can be included, which require the optional
run of the tool dropEst within the cellranger pipeline (see below).

(A) Starting from a processed tenx count matrix (and a metadata.tsv file).

Typically involves linking "dataset.dir" subfolders from a
pipeline_cellranger.py run.

A folder containing the expression matrix (market exchange format)
and metadata.tsv file should be linked into a "data.dir" subfolder.
The folder names must end with ".dir".

e.g.

$ ls data.dir
agg.dir               d1_control_mono.dir  d2_butyrate_mono.dir
d1_butyrate_mono.dir  d1_tmp195_mono.dir   d2_control_mono.dir

$ ls data.dir/agg.dir/
barcodes.tsv  genes.tsv  matrix.mtx  metadata.tsv

(B) Starting from a saved seurat object.

The pipeline can run downstream analysis on a saved seurat object (RDS
format) on which qc, data normalisation, selection of variable genes and
dimension reduction has been performed.

Each sample should be placed (or linked) as a "begin.rds" file in a directory
ending with ".seurat.dir", e.g.

wildtype.seurat.dir/begin.rds
knockout.seurat.dir/begin.rds
aggregated.seurat.dir/begin.rds

The supplied object must contain an RNA assay with populated "data" and "scale.data" slots for all genes (i.e. you need to run NormlizeData and ScaleData on the RNA assay).

The seurat "JackStraw" and "ScoreJackStraw" functions must have run on the reduced dimensions (e.g. pca) of the default assay of the saved object.
[<0;19;19M
The default assay of the saved object will be used for cell-level analyses such as cluster discovery, computation of tSNE/umap coordinates and pseudotime. Hence, if, for example, integration has been performed, "integrated" should be set as the default assay. For gene level analyses the pipeline will always use the RNA assay regardless of the default assay.

(Optional - velocity) Starting from aggregated dropEst output matrix.

Typically involves linking "dropEst-datasets.dir/sample.layers" subfolders from the
pipeline_cellranger.py run.

Similar to (A), a "data.velocity.dir" folder has to be created with
subfolders of the different conditions. The folder names must end
with ".dir" and folder structure should correspond to (A).


Dependencies
------------

This pipeline requires:

* cgat-core: https://github.com/cgat-developers/cgat-core
* cellranger: https://support.10xgenomics.com/single-cell-gene-expression/
* picard tools (optional): https://broadinstitute.github.io/picard/
* R & various packages.
* Latex.


Pipeline output
===============

For each sample and each combination of paramters the following is generated
in the "report.dir" subfoler:

* A pdf summary report
* A pdf gene expression report (arbitrary sets of genes can be specified)
* An excel table of cluster marker genes
* An excel table of cluster-enriched genesets
* Optionally an excel table of genes differentially expressed within cluster
* Optionally an excel table of genesets enriched in amongst genes
differentially expressed within-cluster

Intermediate results files are also retained in the per-sample directories.

"""

from ruffus import *
from ruffus.combinatorics import *
from pathlib import Path
import sys
import os
import re
import shutil
import glob
import sqlite3
import numpy as np
import pandas as pd
from scipy.stats.mstats import gmean
import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as IOTools

import math
import importlib
import textwrap
import yaml

# import local pipeline utility functions
from pipeline_utils import templates
from pipeline_utils import resources
from pipeline_utils import TASK

# -------------------------- < parse parameters > --------------------------- #

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])

# set the location of the tenx code directory
if "tenx_dir" not in PARAMS.keys():
    PARAMS["tenx_dir"] = Path(__file__).parents[1]
else:
    raise ValueError("Could not set the location of the tenx code directory")


# ----------------------- < pipeline configuration > ------------------------ #

# handle pipeline configuration
if len(sys.argv) > 1:
        if(sys.argv[1] == "config") and __name__ == "__main__":
                    sys.exit(P.main(sys.argv))


# ----------------------- < helper functions > ------------------------ #


@files(None, "task.summary.table.tex")
def taskSummary(infile, outfile):
    '''Make a summary of optional tasks that will be run'''

    tasks, run = [], []

    for k,v in PARAMS.items():
        if k.startswith("run_"):
            tasks.append(k[4:])
            run.append(str(v))

    tab = pd.DataFrame(list(zip(tasks,run)),columns=["task","run"])
    print(tab)

    tab.to_latex(buf=outfile, index=False)


# ########################################################################### #
# ############ construct one seurat object per input matrix ################# #
# ########################################################################### #

@transform("data.dir/*.dir",
           regex(r".*/(.*).dir"),
           r"\1.seurat.dir/create.seurat.object.sentinel")
def createSeuratObject(infile, outfile):
    '''Setup the Seurat object and save it in RDS format.

       The Rscript "seurat_begin.R" reads in the raw data, performs
       QC filtering, removal of unwanted varation, identification of
       variable genes and PCA-based dimension reduction.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    metadata = os.path.join(infile, "metadata.tsv.gz")

    if PARAMS["subsetcells_active"]:

        cells_to_use = PARAMS["subsetcells_" + spec.sample_name]

        if PARAMS["subsetcells_type"] == "barcode_list":

            if not os.path.exists(cells_to_use) or cells_to_use == "use.all":
                raise ValueError("Invalid cell subsetting parameter"
                                 " specification")

            subset = '''--subsetcells=%(cells_to_use)s
                     ''' % locals()

        elif PARAMS["subsetcells_type"] == "factor":

            subset_factor = PARAMS["subsetcells_factor"]

            subset = '''--subsetfactor=%(subset_factor)s
                        --subsetlevel=%(cells_to_use)s
                     ''' % locals()

        else:
            raise ValueError("Unknown type of subsetting requested")

    else:
        subset = ""

    # Deal with excludelist
    if PARAMS["excludelist_active"]:
        excludelist = '''--excludelist=%(excludelist_path)s
                    ''' % PARAMS
    else:
        excludelist = ""


    if PARAMS["qc_seed"] != "none":
        seed = '''--seed=%(qc_seed)s''' % PARAMS
    else:
        seed = ""

    # Turn Python boolean into R logical
    downsamplecells = str(PARAMS["qc_downsamplecells"]).upper()

    # Deal with maxcount option
    if PARAMS["qc_maxcount"] != "none":
        maxcount = '''--qcmaxcount=%(qc_maxcount)s''' % PARAMS
    else:
        maxcount = ""

    # set the job threads and memory

    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_high"],
        cpu=PARAMS["resources_numcores"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_begin.R
                   --tenxdir=%(infile)s
                   --matrixtype=%(matrix_type)s
                   --project=%(sample_name)s
                   --outdir=%(outdir)s
                   --groupby=%(qc_groupby)s
                   --mingenes=%(qc_initial_mingenes)s
                   --mincells=%(qc_mincells)s
                   --qcmingenes=%(qc_mingenes)s
                   --qcminpercentmito=%(qc_minpercentmito)s
                   --qcmaxpercentmito=%(qc_maxpercentmito)s
                   --metadata=%(metadata)s
                   --downsamplecells=%(downsamplecells)s
                   --numcores=%(resources_numcores)s
                   --memory=%(r_memory)s
                   --plotdirvar=sampleDir
                   %(subset)s
                   %(excludelist)s
                   %(seed)s
                   %(maxcount)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@active_if(PARAMS["run_explore_hvg_and_cell_cycle"])
@transform(createSeuratObject,
           regex(r"(.*).dir/create.seurat.object.sentinel"),
           r"\1.dir/explore.hvg.and.cell.cycle.sentinel")
def exploreHvgAndCellCycle(infile, outfile):
    '''Make plots of the HVG identified using the selected methods.

       If cell cycle genes are supplied cell cycle effects will be
       visualised in PCA space.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    metadata = os.path.join(infile, "metadata.tsv.gz")

    # Deal with cell-cycle options
    if (PARAMS["cellcycle_sgenes"]=="cc.genes" and
        PARAMS["cellcycle_g2mgenes"]=="cc.genes"):
        cell_cycle_genes = ""

    elif ( os.path.isfile(PARAMS["cellcycle_sgenes"]) and
         os.path.isfile(PARAMS["cellcycle_g2mgenes"]) ):

        cell_cycle_genes = '''--sgenes=%(cellcycle_sgenes)s
                              --g2mgenes=%(cellcycle_g2mgenes)s
                           ''' % PARAMS
    else:
        raise ValueError("Cell cycle genes incorrectly specified")


    if PARAMS["regress_cellcycle"] != "none":
        cell_cycle_regress = '''--cellcycle=%(regress_cellcycle)s
                             ''' % PARAMS
    else:
        cell_cycle_regress = ""

    if PARAMS["qc_seed"] != "none":
        seed = '''--seed=%(qc_seed)s''' % PARAMS
    else:
        seed = ""

    # set the job threads and memory
    job_threads, job_memory, r_memory =  TASK.get_resources(
        memory=PARAMS["resources_memory_extreme"],
        cpu=PARAMS["resources_numcores"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_explore_hvg_and_cell_cycle.R
                   --seuratobject=%(seurat_object)s
                   --species=%(annotation_species)s
                   --project=%(sample_name)s
                   --outdir=%(outdir)s
                   --normalizationmethod=%(normalization_method)s
                   --latentvars=%(regress_latentvars)s
                   --modeluse=%(regress_modeluse)s
                   --vargenesmethod=%(vargenes_method)s
                   --regressgenes=%(regress_genes)s
                   --topgenes=%(vargenes_topgenes)s
                   --sdcutoff=%(vargenes_sdcutoff)s
                   --xlowcutoff=%(vargenes_xlowcutoff)s
                   --xhighcutoff=%(vargenes_xhighcutoff)s
                   --minmean=%(vargenes_minmean)s
                   --vargenespadjust=%(vargenes_padjust)s
                   --numcores=%(resources_numcores)s
                   --memory=%(r_memory)s
                   --plotdirvar=sampleDir
                   %(seed)s
                   %(cell_cycle_genes)s
                   %(cell_cycle_regress)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@transform(createSeuratObject,
           regex(r"(.*).dir/create.seurat.object.sentinel"),
           r"\1.dir/normalise.and.scale.sentinel")
def normaliseAndScale(infile, outfile):
    '''This task performs only the normalisation and scaling steps.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    metadata = os.path.join(infile, "metadata.tsv.gz")

    # Deal with cell-cycle options
    if ( os.path.isfile(PARAMS["cellcycle_sgenes"]) and
         os.path.isfile(PARAMS["cellcycle_g2mgenes"]) ):

        cell_cycle_genes = '''--sgenes=%(cellcycle_sgenes)s
                              --g2mgenes=%(cellcycle_g2mgenes)s
                           ''' % PARAMS

    else:
        cell_cycle_genes = ""

    if PARAMS["regress_cellcycle"] != "none":
        cell_cycle_regress = '''--cellcycle=%(regress_cellcycle)s
                             ''' % PARAMS
    else:
        cell_cycle_regress = ""

    if PARAMS["qc_seed"] != "none":
        seed = '''--seed=%(qc_seed)s''' % PARAMS
    else:
        seed = ""

    # set the job threads and memory
    job_threads, job_memory, r_memory =  TASK.get_resources(
        memory=PARAMS["resources_memory_extreme"],
        cpu=PARAMS["resources_numcores"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_normalise_and_scale.R
                   --species=%(annotation_species)s
                   --seuratobject=%(seurat_object)s
                   --project=%(sample_name)s
                   --outdir=%(outdir)s
                   --normalizationmethod=%(normalization_method)s
                   --latentvars=%(regress_latentvars)s
                   --modeluse=%(regress_modeluse)s
                   --vargenesmethod=%(vargenes_method)s
                   --regressgenes=%(regress_genes)s
                   --topgenes=%(vargenes_topgenes)s
                   --sdcutoff=%(vargenes_sdcutoff)s
                   --xlowcutoff=%(vargenes_xlowcutoff)s
                   --xhighcutoff=%(vargenes_xhighcutoff)s
                   --minmean=%(vargenes_minmean)s
                   --vargenespadjust=%(vargenes_padjust)s
                   --numcores=%(resources_numcores)s
                   --memory=%(r_memory)s
                   --plotdirvar=sampleDir
                   %(seed)s
                   %(cell_cycle_genes)s
                   %(cell_cycle_regress)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@transform(normaliseAndScale,
           regex(r"(.*).dir/normalise.and.scale.sentinel"),
           r"\1.dir/seurat.pca.sentinel")
           
def seuratPCA(infile, outfile):
    '''
       Perform the PCA task
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    metadata = os.path.join(infile, "metadata.tsv.gz")

    if PARAMS["qc_seed"] != "none":
        seed = '''--seed=%(qc_seed)s''' % PARAMS
    else:
        seed = ""

    # Turn Python boolean into R logical
    downsamplecells = str(PARAMS["qc_downsamplecells"]).upper()

    if PARAMS["run_jackstraw"]:
        jackstraw = "--jackstraw"
    else:
        jackstraw = ""

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_extreme"],
        cpu=PARAMS["resources_numcores"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_pca.R
                   --seuratobject=%(seurat_object)s
                   --outdir=%(outdir)s
                   --normalizationmethod=%(normalization_method)s
                   --vargenesmethod=%(vargenes_method)s
                   --jackstrawnumreplicates=%(jackstraw_n_replicate)s
                   --numcores=%(resources_numcores)s
                   --memory=%(r_memory)s
                   --plotdirvar=sampleDir
                   %(jackstraw)s
                   %(seed)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


# ################################################################### #
# ############### Predict cell-types using singleR ################### #
# #################################################################### #


@active_if(PARAMS["headstart_seurat_object"])
@transform(os.path.join(str(PARAMS["headstart_path"]),
                        "*.seurat.dir/begin.rds"),
           regex(r".*/(.*).seurat.dir/begin.rds"),
                 r"\1.seurat.dir/begin.rds")
def headstart(infile, outfile):
    '''
    link in the begin.rds objects
    '''

    outdir = os.path.dirname(outfile)

    if not os.path.exists(outdir):
        os.mkdir(outdir)

    if os.path.exists(infile):

        os.symlink(os.path.relpath(infile, start=outdir),
                   outfile)

    else:
        raise ValueError("Headstart seurat object path not found")


# ################################################################### #
# ############### Predict cell-types using singleR ################### #
# #################################################################### #

def genSingleRjobs():
    '''generate the singleR jobs'''

    seurat_objects = glob.glob("*.seurat.dir/begin.rds")

    references = [x.strip() for x in PARAMS["singleR_reference"].split(",")]

    for seurat_object in seurat_objects:

        seurat_dir = os.path.dirname(seurat_object)

        for reference in references:

            yield [seurat_object,
                   os.path.join(seurat_dir,
                                "singleR.dir",
                                reference + ".ref.dir",
                                "singleR.sentinel")]


@active_if(PARAMS["run_singleR"])
@follows(seuratPCA, headstart)
@files(genSingleRjobs)
def singleR(infile, outfile):
    '''Perform cell identity prediction on a saved seurat object.
    The reference dataset is chosen by the user.

    The output consists of an rds object containing the prediction
    result ("predictions.rds") and a tsv file containing the predicted
    labels ("labels.tsv.gz")
    '''

    reference = os.path.basename(
        Path(outfile).parents[0]).replace(".ref.dir","")


    if PARAMS["headstart_singleR"]:

        spec, SPEC = TASK.get_vars(infile, outfile, PARAMS,
                                   make_outdir=True)

        source = os.path.join(PARAMS["headstart_path"],
                                     spec.sample_dir,
                              "singleR.dir",
                              reference + ".ref.dir")

        if os.path.exists(source):

            source_files = glob.glob(os.path.join(source,
                                                      "*"))

            for sf in [x for x in source_files if not x.endswith("singleR.sentinel")]:

                if os.path.exists(sf):
                    os.symlink(sf,
                               os.path.join(spec.sample_dir,
                                            "singleR.dir",
                                            reference + ".ref.dir",
                                            os.path.basename(sf)))

                else:
                    raise ValueError("Headstart singleR path not found")

            IOTools.touch_file(outfile)

    else:

        spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)


        # set the job threads and memory
        job_threads, job_memory, r_memory = TASK.get_resources(
            memory=PARAMS["resources_memory_extreme"],
            cpu=PARAMS["singleR_workers"])

        statement = '''Rscript %(tenx_dir)s/R/singleR_run.R
                           --seuratobject=%(infile)s
                           --reference=%(reference)s
                           --workers=%(singleR_workers)s
                           --outdir=%(outdir)s

                           &> %(log_file)s
                           ''' % dict(PARAMS, **SPEC, **locals())

        P.run(statement)
        IOTools.touch_file(outfile)


@transform(singleR,
           formatter("(.sentinel)$"),
           "{path[0]}/"
           "{basename[0]}.plot.sentinel")
def plotSingleR(infile, outfile):
    '''Make the singleR heatmap'''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    predictions = os.path.join(spec.indir,
                               "predictions.rds")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"])

    statement = '''Rscript %(tenx_dir)s/R/singleR_plots.R
                       --seuratobject=%(seurat_object)s
                       --predictions=%(predictions)s
                       --outdir=%(outdir)s
                       --pdf=%(plot_pdf)s
                       &> %(log_file)s
                       ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@active_if(PARAMS["singleR_plot_extra"])
@transform(singleR,
           formatter("(.sentinel)$"),
           "{path[0]}/"
           "{basename[0]}.plot.sentinel")
def plotExtraSingleR(infile, outfile):
    '''Make extra plots to explore the singleR annotations
       (slower, plots not currently included in the report)
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    predictions = os.path.join(spec.indir,
                               "predictions.rds")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"])

    statement = '''Rscript %(tenx_dir)s/R/singleR_extra_plots.R
                       --seuratobject=%(seurat_object)s
                       --predictions=%(predictions)s
                       --outdir=%(outdir)s

                       &> %(log_file)s
                       ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)



# ########################################################################### #
# ############ Begin per-parameter combination analysis runs ################ #
# ########################################################################### #


@follows(seuratPCA, headstart)
@transform("*.seurat.dir/begin.rds",
           regex(r"(.*)/begin.rds"),
           r"\1/export_for_python.sentinel")
def exportForPython(infile, outfile):
    '''
    Export data matrices, embeddings etc for analysis with
    python (e.g. with scanpy).

    When Seurat is fully hdf5/anndata/loom compliant we may simply switch
    to storing the Seurat results in hdf5/annadata/loom rather than .rds
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)


    if PARAMS["headstart_export_for_python"]:

        source_folder = os.path.join(PARAMS["headstart_path"],
                                     spec.sample_dir)

        source_files = ["embedding.*.tsv.gz",
                        "export_for_python.*",
                        "metadata.tsv.gz",
                        "features.tsv.gz",
                        "barcodes.tsv.gz",
                        "assay.*.data.tsv.gz",
                        "sig_comps.tsv"]

        source_paths = [ os.path.join(source_folder, x)
                         for x in source_files ]

        for source in source_paths:
            if "*" in source:
                files = glob.glob(source)
            else:
                files = [ source ]

            for sf in files:
                if os.path.exists(sf):
                    os.symlink(os.path.relpath(sf,
                                               start=spec.sample_dir),
                               os.path.join(spec.sample_dir,
                                            os.path.basename(sf)))
                else:
                    pass
                    # TODO:  error catching needed here.
                    #raise ValueError("Headstart source path: " + sf +\
                    #                 " not found")

    else:

        reductiontype = PARAMS["dimreduction_method"]

        if bool(re.search("sig", str(PARAMS["runspecs_n_components"]))) :
            comp="--usesigcomponents=TRUE"
        else :
            comp="--usesigcomponents=FALSE"

        export_counts = "FALSE"
        export_data = "FALSE"

        if PARAMS["run_phate"]:
            export_scaled_data = "TRUE"
        else:
            export_scaled_data = "FALSE"

        # set the job threads and memory
        job_threads, job_memory, r_memory = TASK.get_resources(
            memory=PARAMS["resources_memory_standard"])

        statement = '''Rscript %(tenx_dir)s/R/export_for_python.R
                       --seuratobject=%(seurat_object)s
                       --reductiontype=%(reductiontype)s
                       --counts=%(export_counts)s
                       --data=%(export_data)s
                       --scaled=%(export_scaled_data)s
                       --outdir=%(outdir)s
                       %(comp)s
                       &> %(log_file)s
                    ''' % dict(PARAMS, **SPEC, **locals())

        P.run(statement)
        IOTools.touch_file(outfile)


def genAnndata():

    components_str = str(PARAMS["runspecs_n_components"])
    components = components_str.strip().replace(" ", "").split(",")
    samples = glob.glob("*.seurat.dir")

    outname = "anndata.sentinel"

    for sample in samples:

        infile = os.path.join(sample, "begin.rds")

        for comps in components:

            outdir = "components." + comps + ".dir"
            subdir = "anndata.dir"

            outfile = os.path.join(sample, outdir, subdir, outname)
            yield [infile, outfile]


@follows(exportForPython)
@files(genAnndata)
def anndata(infile, outfile):
    '''
       make an anndata object with a knn graph
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    reductiontype = PARAMS["dimreduction_method"]

    reduced_dims_matrix_file = os.path.join(spec.sample_dir,
                                            "embedding." + reductiontype + ".tsv.gz")

    barcode_file = os.path.join(spec.sample_dir,
                                "barcodes.tsv.gz")

    SPEC["components"] = outfile.split("/")[1][len("components."):-len(".dir")]

    if SPEC["components"] == "sig":
#        raise ValueError("Use of significant components not yet supported")
        sigcomps = os.path.join(spec.sample_dir, "sig_comps.tsv")
        comps = pd.read_table(sigcomps, header=None)
        comps = comps[comps.columns[0]].tolist()
        comps = ','.join([ str(item) for item in comps])
    else :
        comps = list(range (1, int(spec.components)+1))
        comps = ','.join([ str(item) for item in comps])

    # set the job threads and memory

    if PARAMS["neighbors_full_speed"]:
        full_speed = "--fullspeed"
    else:
        full_speed = ""

    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_extreme"])

    statement = '''python %(tenx_dir)s/python/make_anndata.py
                   --reduced_dims_matrix_file=%(reduced_dims_matrix_file)s
                   --barcode_file=%(barcode_file)s
                   --outdir=%(outdir)s
                   --comps=%(comps)s
                   --method=%(neighbors_method)s
                   --threads=%(neighbors_threads)s
                   --k=%(neighbors_n_neighbors)s
                   --metric=%(neighbors_metric)s
                   %(full_speed)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


def genScanpyClusterJobs():

    components_str = str(PARAMS["runspecs_n_components"])
    components = components_str.strip().replace(" ", "").split(",")
    samples = glob.glob("*.seurat.dir")

    if PARAMS["runspecs_cluster_resolutions"]:
        resolutions = [ x.strip() for x in
                        str(PARAMS["runspecs_cluster_resolutions"]).split(",") ]

    outname = "scanpy.clusters.sentinel"

    for sample in samples:

        infile = os.path.join(sample, "begin.rds")

        for comps in components:

            outdir = "components." + comps + ".dir"

            if PARAMS["runspecs_predefined_clusters"] :

                subdir = "cluster.predefined.dir"
                outfile = os.path.join(sample, outdir, subdir, outname)
                yield [infile, outfile]

            if PARAMS["runspecs_cluster_resolutions"]:
                for resolution in resolutions:

                    subdir = "cluster." + resolution + ".dir"
                    outfile = os.path.join(sample, outdir, subdir, outname)
                    yield [infile, outfile]



#@active_if(PARAMS["run_paga"])
#@active_if(PARAMS["cluster_predefined"]==False)
@follows(anndata)
@files(genScanpyClusterJobs)
def scanpyCluster(infile, outfile):
    '''
       discover clusters using scanpy.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"])

    if not "cluster.predefined.dir" in spec.outdir:

        statement = '''python %(tenx_dir)s/python/run_cluster.py
                   --anndata=%(anndata)s
                   --algorithm=%(cluster_algorithm)s
                   --resolution=%(resolution)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

        P.run(statement)

    IOTools.touch_file(outfile)


@transform(scanpyCluster,
           regex(r"(.*)/scanpy.clusters.sentinel"),
           r"\1/cluster.sentinel")
def cluster(infile, outfile):
    '''
    post-process the clustering result
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"])

    scanpy_cluster_tsv = infile.replace(".sentinel",".tsv.gz")

    if "cluster.predefined.dir"  in spec.indir:
        predefined='--predefined=' + PARAMS["runspecs_predefined_clusters"]
    else:
        predefined=""

    statement = '''Rscript %(tenx_dir)s/R/scanpy_post_process_clusters.R
                   --seuratobject=%(seurat_object)s
                   --clusters=%(scanpy_cluster_tsv)s
                   --algorithm=%(cluster_algorithm)s
                   %(predefined)s
                   --mincells=10
                   --outdir=%(outdir)s
                   > %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@active_if(PARAMS["run_compare_clusters"])
@transform(cluster,
           regex(r"(.*)/(.*)/(.*)/cluster.sentinel"),
           r"\1/\2/\3/compare.clusters.sentinel")
def compareClusters(infile, outfile):
    '''Perform clustering on a saved seurat object.

       The single-cells are clustered using the given number of PCA components,
       resolution and alogorithm.

       Clusters are written to "cluster_ids.tsv".
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    reductiontype = PARAMS["dimreduction_method"]

    if(spec.components=="sig"):
        comp="--usesigcomponents=TRUE"
    else:
        comp="--components=%(components)s" % SPEC

    if spec.resolution == "predefined":

        # TODO: Fix.
        cluster_file = sample + ".cluster_ids.rds"

        if os.path.exists(cluster_file):
            predefined = "--predefined=%(cluster_file)s" % locals()

        else:
            raise ValueError("Predefined cluster assignment file (%(cluster_file)s) not found" % locals())

    else:
        predefined = ""

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_compare_clusters.R
                   --seuratobject=%(seurat_object)s
                   --clusterids=%(cluster_ids)s
                   %(comp)s
                   --outdir=%(outdir)s
                   --reductiontype=%(reductiontype)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


# ########################################################################### #
# ############### compare the clusters across resolutions ################### #
# ########################################################################### #

nresolutions = len(str(PARAMS["runspecs_cluster_resolutions"]).split(","))

@active_if(nresolutions > 1)

@follows(cluster)
@transform(anndata,
           regex(r"(.*)/(.*)/anndata.dir/anndata.sentinel"),
           r"\1/\2/clustree.sentinel")
def clustree(infile, outfile):
    '''
       Run clustree.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    id_files = [ os.path.join(spec.outdir,
                              "cluster." + r + ".dir",
                              "cluster_ids.rds")
                 for r in spec.resolutions ]

    res_str = ",".join(spec.resolutions)
    id_files_str = ",".join(id_files)

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_clustree.R
                   --resolutions=%(res_str)s
                   --clusteridfiles=%(id_files_str)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)





@active_if(PARAMS["run_paga"])
@follows(anndata)
@transform(cluster,
           regex(r"(.*)/(.*)/(.*)/cluster.sentinel"),
           r"\1/\2/\3/paga.dir/paga.sentinel")
def paga(infile, outfile):
    '''
       Run partition-based graph abstraction (PAGA)
       see: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1663-x
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_high"])

    statement = '''python %(tenx_dir)s/python/run_paga.py
                   --anndata=%(anndata)s
                   --outdir=%(outdir)s
                   --cluster_assignments=%(cluster_assignments)s
                   --cluster_colors=%(cluster_colors)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@active_if(PARAMS["run_phate"])
@follows(exportForPython,cluster)
@transform(anndata,
           regex(r"(.*)/(.*)/anndata.dir/anndata.sentinel"),
           r"\1/\2/phate.dir/phate.sentinel")
def phate(infile, outfile):
    '''
       Run the PHATE dimension reduction alogorithm
       see: https://www.nature.com/articles/s41587-019-0336-3
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    id_files = [ os.path.join(spec.component_dir,
                              "cluster." + r + ".dir",
                              "cluster_assignments.tsv.gz")
                 for r in spec.resolutions ]


    color_files = [ os.path.join(spec.component_dir,
                             "cluster." + r + ".dir",
                              "cluster_colors.tsv")
                 for r in spec.resolutions ]


    cluster_assignment_files = ",".join(id_files)

    cluster_color_files = ",".join(color_files)

    cluster_resolutions = ",".join(spec.resolutions)

    barcode_file = os.path.join(spec.sample_dir,
                                "barcodes.tsv.gz")

    if PARAMS["phate_assay"] == "reduced.dimensions":

        embeddings = ".".join(["embedding",
                               PARAMS["dimreduction_method"],
                               "tsv.gz"])

        assay_data = os.path.join(spec.sample_dir, embeddings)

    elif PARAMS["phate_assay"] == "scaled.data":
        assay_data = os.path.join(spec.sample_dir, "assay.scale.data.tsv.gz")

    else:
        raise ValueError("PHATE assay not recognised")


    k = PARAMS["phate_k"]

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"])

    statement = '''python %(tenx_dir)s/python/run_phate.py
                   --data=%(assay_data)s
                   --assay=%(phate_assay)s
                   --barcode_file=%(barcode_file)s
                   --outdir=%(outdir)s
                   --resolution=%(cluster_resolutions)s
                   --cluster_assignments=%(cluster_assignment_files)s
                   --cluster_colors=%(cluster_color_files)s
                   --k=%(k)s
                   --gif=%(phate_gif)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


# ########################################################################### #
# ############### UMAP analysis and related plots ########################### #
# ########################################################################### #

@transform(anndata,
           regex(r"(.*)/(.*)/anndata.sentinel"),
           r"\1/umap.dir/umap.sentinel")
def UMAP(infile, outfile):
    '''
    Run the UMAP analysis on a saved anndata object.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_high"],
        cpu=2)

    mindists = [x.strip() for x in PARAMS["umap_mindists"].split(",")]

    statements = []
    for mindist in mindists:

        mindist_log_file = outfile.replace(".sentinel",
                                           "." + mindist + ".log")

        statement = '''python %(tenx_dir)s/python/run_umap.py
                             --anndata=%(anndata)s
                             --mindist=%(mindist)s
                             --outdir=%(outdir)s
                             &> %(mindist_log_file)s
                     ''' % dict(PARAMS, **SPEC, **locals())

        statements.append(statement)

    P.run(statements)
    IOTools.touch_file(outfile)




# ########################################################################### #
# ############################## Diffusion maps ############################# #
# ########################################################################### #

@active_if(PARAMS["run_diffusionmap"])
@transform(anndata,
           regex(r"(.*)/(.*)/anndata.dir/anndata.sentinel"),
           r"\1/\2/diffusionmap.dir/dm.sentinel")
def diffusionMap(infile, outfile):
    '''
    Run the diffusion map analysis on a saved seurat object.
    '''

    ## TODO: fix plotting flow

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    if(spec.components=="sig"):
        comp="--usesigcomponents=TRUE"
    else:
        comp="--components=%(components)s" % SPEC

    if PARAMS["diffusionmap_usegenes"]:
        usegenes="--usegenes=TRUE"
    else:
        usegenes="--usegenes=FALSE"

    SPEC["outname"] = outfile.replace(".sentinel", ".tsv.gz")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_high"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_compute_dm.R
                             --seuratobject=%(seurat_object)s
                             %(usegenes)s
                             %(comp)s
                             --reductiontype=%(dimreduction_method)s
                             --maxdim=%(diffusionmap_maxdim)s
                             --outfile=%(outname)s
                             &> %(log_file)s
                          ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


# ########################################################################### #
# ####################### Known gene violin plots ########################### #
# ########################################################################### #

@active_if(PARAMS["run_knownmarkers"])
@transform(cluster,
           regex(r"(.*)/(.*)/(.*)/cluster.sentinel"),
           r"\1/\2/\3/known.markers.dir/known.markers.sentinel")
def knownMarkerViolins(infile, outfile):
    '''
       Make per-cluster violin plots from a given set of known marker genes.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    if not os.path.exists(PARAMS["knownmarkers_file"]):
        raise ValueError("The specified known markers file does not exist")

    outprefix = outfile.replace(".sentinel", "")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"])

    statement = '''Rscript %(tenx_dir)s/R/plot_violins.R
                       --genetable=%(knownmarkers_file)s
                       --seuratobject=%(seurat_object)s
                       --seuratassay=RNA
                       --clusterids=%(cluster_ids)s
                       --outprefix=%(outprefix)s
                       --plotdirvar=knownmarkersDir
                       &> %(log_file)s
        ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


# ########################################################################### #
# ################## Set the DR visualisation method ######################## #
# ########################################################################### #

# Used to show clusters, factors of interest and gene expression levels
# in various downstream functions

# if PARAMS["dimreduction_visualisation"].lower() == "tsne":
#     RDIMS_VIS_TASK = tSNE
#     RDIMS_VIS_METHOD = "tsne"
#     RDIMS_VIS_COMP_1 = "tSNE_1"
#     RDIMS_VIS_COMP_2 = "tSNE_2"

# elif PARAMS["dimreduction_visualisation"].lower() == "umap":
RDIMS_VIS_TASK = UMAP
RDIMS_VIS_METHOD = "umap"
RDIMS_VIS_COMP_1 = "UMAP_1"
RDIMS_VIS_COMP_2 = "UMAP_2"

# else:
#     raise ValueError('dimreduction_visualisation must be either "tsne" or "umap"')


# ########################################################################### #
# ########################### RNA Velocity ################################## #
# ########################################################################### #

@active_if(PARAMS["run_velocity"])
@follows(paga)
@transform(RDIMS_VIS_TASK,
           regex(r"(.*)/(.*)/(.*).sentinel"),
           r"\1/velocity.dir/scvelo.sentinel")
def scvelo(infile, outfile):
    '''
       Plot the RNA velocity.
       This analysis is highly parameterised and different configurations can
       suggest different interpretations of the data.
    '''

    # TODO: fix plotting flow

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)


    id_files = [ os.path.join(spec.component_dir,
                              "cluster." + r + ".dir",
                              "cluster_assignments.tsv.gz")
                 for r in spec.resolutions ]


    color_files = [ os.path.join(spec.component_dir,
                             "cluster." + r + ".dir",
                              "cluster_colors.tsv")
                 for r in spec.resolutions ]


    cluster_assignment_files = ",".join(id_files)

    cluster_color_files = ",".join(color_files)

    cluster_resolutions = ",".join(spec.resolutions)


    # if RDIMS_VIS_METHOD == "tsne":
    #     rdims_table = infile.replace(
    #         "sentinel", str(PARAMS["tsne_perplexity"]) + ".tsv")
    # elif RDIMS_VIS_METHOD == "umap":

    rdims_vis_method = RDIMS_VIS_METHOD
    rdim1 = RDIMS_VIS_COMP_1
    rdim2 = RDIMS_VIS_COMP_2

    rdims_table = os.path.join(spec.component_dir,
                               "umap.dir",
                               "umap." + str(PARAMS["umap_mindist"]) + ".tsv.gz")

    sample = infile.split(".seurat.dir")[0]


    if PARAMS["velocity_matrix_type"] == "loom":

        loom_file = os.path.join("data.velocity.dir",
                                 sample + ".loom")

        velocity_data = "--loom=" + loom_file

    elif PARAMS["velocity_matrix_type"] == "dropest":

        layers_dir = os.path.join("data.velocity.dir",
                             sample + ".dir")

        velocity_data = "--dropest_dir=" + layers_dir

        if not os.path.exists(os.path.join(layers_dir,"exons.mtx.gz")):
            raise ValueError("dropest output not found. Please check that you "
                             "have symlinked the .layers directory")

    else:
        raise ValueError('velocity matrix type must be "loom" or "dropest"')

    runs = { "default": { "method": rdims_vis_method,
                          "table": rdims_table,
                          "rdim1": rdim1, "rdim2": rdim2 }}

    if PARAMS["run_paga"]:


        pagafdg_tables = ",".join([os.path.join(
                spec.component_dir,
                "cluster." + x + ".dir",
                "paga.dir",
                "paga_init_fa2.tsv.gz") for x in spec.resolutions])

        runs["pagafdg"] = {"method": "paga_fdg",
                           "table": pagafdg_tables,
                           "rdim1": "FA1", "rdim2": "FA2"}

    if PARAMS["run_phate"]:

        phate_table = os.path.join(spec.component_dir,
                                   "phate.dir",
                                   "phate.tsv.gz")

        runs["phate"] = {"method": "phate",
                         "table": phate_table,
                         "rdim1": "PHATE1", "rdim2": "PHATE2"}

    statements = []

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_high"])

    for run, details in runs.items():

        r_method, r_tab, r_1, r_2 = details["method"], details["table"], \
                                    details["rdim1"], details["rdim2"]

        this_log_file = spec.log_file.replace(".log",
                                              "." + r_method + ".log")

        s = '''python %(tenx_dir)s/python/run_scvelo.py
                   %(velocity_data)s
                   --outdir=%(outdir)s
                   --resolution=%(cluster_resolutions)s
                   --cluster_assignments=%(cluster_assignment_files)s
                   --cluster_colors=%(cluster_color_files)s
                   --rdim_method=%(r_method)s
                   --rdims=%(r_tab)s
                   --rdim1=%(r_1)s
                   --rdim2=%(r_2)s
                &> %(this_log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

        statements.append(s)

    P.run(statements)
    IOTools.touch_file(outfile)


# ########################################################################### #
# ###### Visualise gene expression across cells in reduced dimensions ####### #
# ########################################################################### #

@transform(RDIMS_VIS_TASK,
           regex(r"(.*)/(.*)/(.*)/(.*).sentinel"),
           add_inputs(exportForPython),
           r"\1/\2/rdims.visualisation.dir/plot.rdims.factor.sentinel")
def plotRdimsFactors(infiles, outfile):
    '''
    Visualise factors of interest on the projection.
    '''

    infile, export_sentinel = infiles

    metadata_table = os.path.join(os.path.dirname(export_sentinel),
                                  "metadata.tsv.gz")

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    rdims_table = infile.replace(".sentinel",
                                 "." + str(PARAMS["umap_mindist"]) + ".tsv.gz")

    color_factors = []

    if PARAMS["plot_qcvars"] is not None:
        color_factors += [x.strip() for x in
                          PARAMS["plot_qcvars"].split(",")]

    if PARAMS["plot_groups"] is not None:
        color_factors += [x.strip() for x in
                          PARAMS["plot_groups"].split(",")]

    if PARAMS["plot_subgroup"] is not None:
        color_factors += [x.strip() for x in
                          PARAMS["plot_subgroup"].split(",")]

    color_factors = [ x for x in color_factors if x != "cluster" ]

    # ensure list is unique whilst preserving order.
    color_factors = list(dict.fromkeys(color_factors))

    color_factors = "--colorfactors=" + ",".join(color_factors)

    if PARAMS["plot_shape"] is not None:
        shape_factors = "--shapefactor=%(plot_shape)s" % PARAMS
    else:
        shape_factors = ""

    # bring vars into local scope..
    rdims_vis_method = RDIMS_VIS_METHOD
    rdim1 = RDIMS_VIS_COMP_1
    rdim2 = RDIMS_VIS_COMP_2

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"])

    statement = '''Rscript %(tenx_dir)s/R/plot_rdims_factor.R
                   --method=%(rdims_vis_method)s
                   --table=%(rdims_table)s
                   --metadata=%(metadata_table)s
                   --rdim1=%(rdim1)s
                   --rdim2=%(rdim2)s
                   %(shape_factors)s
                   %(color_factors)s
                   --pointsize=%(plot_pointsize)s
                   --pointalpha=%(plot_pointalpha)s
                   --pointpch=%(plot_pointpch)s
                   --pdf=%(plot_pdf)s
                   --outdir=%(outdir)s
                   --plotdirvar=rdimsVisFactorDir
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)

    tex_file = os.path.join(spec.outdir, "plot.rdims.factor.tex")

    with open(tex_file, "w") as tex:
        tex.write("\input{" + os.path.join(spec.outdir) + "/umap}")

    IOTools.touch_file(outfile)


@follows(RDIMS_VIS_TASK)
@transform(cluster,
           regex(r"(.*)/(.*).dir/(.*).dir/(.*).sentinel"),
           r"\1/\2.dir/\3.dir/rdims.visualisation.dir/plot.rdims.cluster.sentinel")
def plotRdimsClusters(infile, outfile):
    '''
    Visualise the clusters on the chosen projection
    '''

    metadata_table = os.path.join(os.path.dirname(infile),
                                  "cluster_assignments.tsv.gz")

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)


    color_factors = "--colorfactors=cluster_id"

    if PARAMS["plot_shape"] is not None:
        shape_factors = "--shapefactor=%(plot_shape)s" % PARAMS
    else:
        shape_factors = ""

    # bring vars into local scope..
    rdims_vis_method = RDIMS_VIS_METHOD
    rdim1 = RDIMS_VIS_COMP_1
    rdim2 = RDIMS_VIS_COMP_2

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"])


    mindists = [x.strip() for x in PARAMS["umap_mindists"].split(",")]

    statements = []
    for mindist in mindists:

        rdims_table = os.path.join(spec.component_dir,
                                   "umap.dir",
                                   "umap." + mindist + ".tsv.gz")

        umap_spec = "umap.mindist_" + mindist

        mindist_log_file = outfile.replace(".sentinel",
                                           "." + mindist + ".log")

        statement = '''Rscript %(tenx_dir)s/R/plot_rdims_factor.R
                   --method=%(umap_spec)s
                   --table=%(rdims_table)s
                   --metadata=%(metadata_table)s
                   --rdim1=%(rdim1)s
                   --rdim2=%(rdim2)s
                   %(shape_factors)s
                   %(color_factors)s
                   --pointsize=%(plot_pointsize)s
                   --pointalpha=%(plot_pointalpha)s
                   --pointpch=%(plot_pointpch)s
                   --pdf=%(plot_pdf)s
                   --outdir=%(outdir)s
                   --plotdirvar=rdimsVisClusterDir
                   &> %(mindist_log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

        statements.append(statement)
    P.run(statements)

    tex_file = os.path.join(spec.outdir, "plot.rdims.factor.tex")

    with open(tex_file, "w") as tex:
        for mindist in mindists:
            tex.write("\input{" + spec.outdir + "/umap.mindist_" + mindist + "}\n")

    IOTools.touch_file(outfile)


@active_if(PARAMS["run_diffusionmap"])
@transform(cluster,
           regex(r"(.*)/(.*).dir/(.*).dir/(.*).sentinel"),
           add_inputs(diffusionMap),
           r"\1/\2.dir/\3.dir/dm.visualisation.dir/plot.dm.cluster.sentinel")
def plotDiffusionMap(infiles, outfile):
    '''
    Run the diffusion map analysis on a saved seurat object.
    '''

    ## TODO: fix plotting flow
    infile, dmsentinel = infiles

    diffusionmap = os.path.join(os.path.dirname(dmsentinel),
                                "dm.tsv.gz")

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    SPEC["outname"] = outfile.replace(".sentinel", ".tsv.gz")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_high"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_plot_dm.R
                             --diffusionmap=%(diffusionmap)s
                             --clusterassignments=%(cluster_assignments)s
                             --outdir=%(outdir)s
                             --plotdirvar=diffmapDir
                             &> %(log_file)s
                          ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)



#@follows(UMAP)
@active_if(PARAMS["run_singleR"])
@product(UMAP,
         formatter("(.sentinel)$"),
         singleR,
         formatter("(.sentinel)$"),
         "{subpath[0][0][1]}/"         #  cluster files, first file  seurat.dir/comp.dir/cluster.dir/clusters
         "singleR.dir/"
         "{subdir[1][0][0]}/"
         "singleR.umap.sentinel")
def plotUmapSingleR(infiles, outfile):
    '''Plot the SingleR primary identity assignments on a UMAP'''

    cluster_sentinel, singleR_sentinel = infiles

    spec, SPEC = TASK.get_vars(cluster_sentinel, outfile, PARAMS)


    singleR_dir = os.path.dirname(singleR_sentinel)

    metadata = os.path.join(singleR_dir, "labels.tsv.gz")

    rdims_table = os.path.join(spec.component_dir,
                               "umap.dir",
                               "umap." + str(PARAMS["umap_mindist"]) + ".tsv.gz")

    # bring vars into local scope..
    rdims_vis_method = RDIMS_VIS_METHOD
    rdim1 = RDIMS_VIS_COMP_1
    rdim2 = RDIMS_VIS_COMP_2

    #outdir = os.path.dirname(outfile)
    #log_file = outfile.replace(".sentinel", ".log")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"])

    statement  = '''Rscript %(tenx_dir)s/R/plot_rdims_factor.R
                        --method=%(rdims_vis_method)s
                        --table=%(rdims_table)s
                        --metadata=%(metadata)s
                        --rdim1=%(rdim1)s
                        --rdim2=%(rdim2)s
                        --colorfactors=pruned.labels
                        --pointsize=%(plot_pointsize)s
                        --pointalpha=%(plot_pointalpha)s
                        --pointpch=%(plot_pointpch)s
                        --pdf=%(plot_pdf)s
                        --outdir=%(outdir)s
                        --plotdirvar=rdimsVisSingleRDir
                        &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@active_if(PARAMS["run_singleR"])
@follows(plotSingleR, plotUmapSingleR)
@transform(UMAP,
           regex(r"(.*)/umap.dir/umap.sentinel"),
           r"\1/singleR.dir/singleR.summary.tex")
def summariseSingleR(infile, outfile):
    '''Collect the single R plots into a section for the report'''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    references = [x.strip() for x in PARAMS["singleR_reference"].split(",")]

    singleR_path = os.path.join(Path(infile).parents[2],
                                "singleR.dir")

    singleR_umap_path = spec.outdir


    latex_path = outfile  # .replace(".sentinel", ".tex")

    with open(latex_path, "w") as tex:

        for reference in references:

            # heatmap
            tex.write(templates.subsection % {"title": reference})
            tex.write("\n")

            heatmap_path = os.path.join(singleR_path,
                                        reference + ".ref.dir",
                                        "singleR_score_heatmap")


            if(os.path.exists(heatmap_path + ".png")):
                heatmap_fig = {"width": "1", "height": "0.9",
                               "path": heatmap_path,
                               "caption": "singleR predictions (" +\
                               reference + ")"}

                tex.write(textwrap.dedent(
                    templates.figure % heatmap_fig))
                tex.write("\n")

            umap_path = os.path.join(singleR_umap_path,
                                        reference + ".ref.dir",
                                        "umap.pruned.labels")

            # umap
            if(os.path.exists(umap_path + ".png")):

                umap_fig = {"width": "1", "height": "0.9",
                            "path": umap_path,
                            "caption": "pruned singleR predictions (" +\
                            reference + ")"}

                tex.write(textwrap.dedent(
                    templates.figure % umap_fig))

                tex.write("\n")

@follows(RDIMS_VIS_TASK)
@active_if(PARAMS["run_exprsreport"] and not PARAMS["run_marker_report"])
@transform(cluster,
           regex(r"(.*)/cluster.sentinel"),
           r"\1/genelists.dir/plot.rdims.genes.sentinel")
def plotRdimsGenes(infile, outfile):
    '''
    Visualise gene expression levels on reduced dimension coordinates

    The @data slot of the seurat object is used.
    '''

    # TODO: test and fix.

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    tex_path = os.path.join(spec.outdir, "plot.rdims.known.genes.tex")

    rdims_vis_method = RDIMS_VIS_METHOD
    rdim1 = RDIMS_VIS_COMP_1
    rdim2 = RDIMS_VIS_COMP_2

    rdims_table = os.path.join(spec.component_dir,
                               "umap.dir",
                               "umap." + str(PARAMS["umap_mindist"]) + ".tsv.gz")


    if PARAMS["exprsreport_genelists"]:
        genelists = glob.glob(
            os.path.join(PARAMS["exprsreport_genelist_dir"], "*.tsv"))

        # if RDIMS_VIS_METHOD == "tsne":
        #     rdims_table = infile.replace(
        #         "sentinel", str(PARAMS["tsne_perplexity"]) + ".tsv")
        # elif RDIMS_VIS_METHOD == "umap":
        rdims_table = infile.replace( ".sentinel", ".tsv")

        # set the job threads and memory
        job_threads, job_memory, r_memory = TASK.get_resources(
            memory=PARAMS["resources_memory_standard"],
            cpu=PARAMS["resources_numcores"])

        for genelist in genelists:

            fname = "plot.rdims.genes." + os.path.basename(genelist) + ".log"
            log_file = os.path.join(outdir, fname)

            if PARAMS["plot_shape"] is not None:
                shape = "--shapefactor=%(plot_shape)s" % PARAMS
            else:
                shape = ""

            statement = '''Rscript %(tenx_dir)s/R/plot_rdims_gene.R
                           --method=%(rdims_vis_method)s
                           --table=%(rdims_table)s
                           --seuratobject=%(seurat_object)s
                           --seuratassay=RNA
                           --rdim1=%(rdim1)s
                           --rdim2=%(rdim2)s
                           %(shape)s
                           --genetable=%(genelist)s
                           --pointsize=%(plot_pointsize)s
                           --pointalpha=%(plot_pointalpha)s
                           --maxcells=%(plot_maxcells)s
                           --outdir=%(outdir)s
                           --pdf=%(plot_pdf)s
                           --plotdirvar=genelistsDir
                           --workers=%(exprsreport_workers)s
                           &> %(log_file)s
                       ''' % dict(PARAMS, **SPEC, **locals())

            P.run(statement)

        # prepare a summary tex snippet for inclusion in the report.

        with(open(tex_path, "w")) as tex:

            for genelist in genelists:

                texf = os.path.join(
                    outdir,
                    "plot.rdims.genes." +
                    os.path.basename(genelist).replace(".tsv", ""))

                gsname = os.path.basename(
                    genelist)[:-len(".tsv")].replace("_", "\\_")

                tex.write(
                    "\\subsection{Expression of known genes: %s}\n" % gsname)
                tex.write(
                    "\\input{%(texf)s}\n" % locals())

            tex.write("\n")

    else:

        with(open(tex_path, "w")) as tex:

            tex.write("No genelists were specified.\n")
            tex.write("\n")

    IOTools.touch_file(outfile)


# ########################################################################### #
# ################# plot per-cluster summary statistics ##################### #
# ########################################################################### #

@follows(exportForPython)
@transform(cluster,
           regex(r"(.*)/cluster.sentinel"),
           r"\1/group.numbers.dir/plot.group.numbers.sentinel")
def plotGroupNumbers(infile, outfile):
    '''
    Plot statistics on cells by group, e.g. numbers of cells per cluster.

    Plots are defined on a case-by-case basis in the yaml.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    metadata_table = os.path.join(spec.sample_dir,
                                  "metadata.tsv.gz")

    # we need the yaml as a dict, so...
    config_file = "pipeline.yml"
    if not os.path.exists(config_file):
        config_file = "%s/pipeline.yml" % os.path.splitext(__file__)[0]
        if not os.path.exists(config_file):
            raise ValueError("Config file not found in plot group numbers")

    with open(config_file) as f:
        params = yaml.load(f, Loader=yaml.FullLoader)

    params = params["summaries"]

    statements = []
    for summary_key in params.keys():

        summary = params[summary_key].copy()

        options = []

        # populate the Rscript options from the yaml dictionary
        for k, v in summary.items():
            if v == "None" or v == None or v == False or k == "title":
                pass
            elif v == True:
                options.append("--" + k)
            elif k in ["xlab", "ylab"]:
                options.append("--" + k + "=\"" + str(v) + "\"")
            else:
                options.append("--" + k + "=" + str(v))

        options = "\n".join(options)

        SPEC["log_file"] = os.path.join(spec.outdir, summary_key + ".log")

        job_threads, job_memory, r_memory = TASK.get_resources(
            memory=PARAMS["resources_memory_min"])

        statement = '''Rscript %(tenx_dir)s/R/plot_group_numbers.R
                   --metadata=%(metadata_table)s
                   --clusters=%(cluster_assignments)s
                   --seuratobject=%(seurat_object)s
                   --seuratassay=RNA
                   --title=%(summary_key)s
                   %(options)s
                   --outdir=%(outdir)s
                   --plotdirvar=groupNumbersDir
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

        statements.append(statement)

    P.run(statements)

    # write out the latex snippet...
    with open(os.path.join(spec.outdir, "number.plots.tex"),"w") as tex:

        for fig in params.keys():

            if("_" in params[fig]["title"]):
               raise ValueError("Underscores are not allowed in the plot"
                                " titles (due to issues with latex...")

            # Add the figures, one per subsection, escaping underscores.
            tex.write(templates.subsection %
                      {"title": params[fig]["title"]})

            tex.write("\n")

            fig_path = os.path.join(spec.outdir, fig)

            if(os.path.exists(fig_path + ".png")):
                fig_spec = {"width": "1", "height": "0.9",
                            "path": fig_path,
                            "caption": params[fig]["title"]
                            }

                tex.write(textwrap.dedent(
                    templates.figure % fig_spec))
                tex.write("\n")

    IOTools.touch_file(outfile)


# ########################################################################### #
# #################### Retrieve geneset annoations ########################## #
# ########################################################################### #

# Retrieve gene annotations and KEGG pathways.
#
# The "ensembl.to.entrez.tsv.gz" table is needed for:
# - adding ensembl gene_ids to the findMarkers results table if s@misc$gene
#   is not set
# - translating ensembl gene_ids to entrez gene_ids for the geneset
#   analysis

@files(None,
       "annotation.dir/genesets.sentinel")
def getGenesetAnnotations(infile, outfile):
    '''Get mappings between Ensembl gene_ids and (i) Entrez ids
       and (ii) KEGG pathways.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS,
                               make_outdir = False)

    if PARAMS["headstart_annotation"]:

        source = os.path.join(PARAMS["headstart_path"],
                                  "annotation.dir")

        if os.path.exists(source):
            os.symlink(os.path.relpath(source,
                                       start="."),
                       "annotation.dir")
        else:
            raise ValueError("Headstart annotation path not found: " + source)

    else:

        if PARAMS["annotation_ensembl_host"] == "default":
            ensembl_host = ""
        else:
            ensembl_host = "--ensemblhost=%(annotation_ensembl_host)s" % PARAMS

        # set the job threads and memory
        job_threads, job_memory, r_memory = TASK.get_resources(
            memory=PARAMS["resources_memory_low"])

        if not os.path.exists(spec.outdir):
            os.mkdir(spec.outdir)

        statement = '''Rscript %(tenx_dir)s/R/fetch_geneset_annotations.R
                     --ensemblversion=%(annotation_ensembl_release)s
                     %(ensembl_host)s
                     --species=%(annotation_species)s
                     --outdir=%(outdir)s
                     &> %(log_file)s
                  ''' % dict(PARAMS, **SPEC, **locals())

        P.run(statement)
        IOTools.touch_file(outfile)


# ########################################################################### #
# ############# Cluster marker identification and visualisation ############# #
# ########################################################################### #

@follows(getGenesetAnnotations)
@transform(cluster,
           regex(r"(.*)/cluster.sentinel"),
           r"\1/cluster.markers.dir/findMarkers.sentinel")
def findMarkers(infile, outfile):
    '''
    Identification of cluster marker genes.

    This analysis is run in parallel for each cluster.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    #cluster_ids = infile.replace(".sentinel","_ids.rds")

    if PARAMS["findmarkers_conserved"]:
        conservedfactor = PARAMS["findmarkers_conserved_factor"]
        conservedpadj = PARAMS["findmarkers_conserved_padj"]
        conserved_options = '''--conservedfactor=%(conservedfactor)s
            --conservedpadj=%(conservedpadj)s
        '''
    else:
        conserved_options = ""

    statements = []

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"],
        cpu=PARAMS["findmarkers_numcores"])

    for i in spec.clusters:

        if str(i) == "911":
            continue

        SPEC["log_file"] = outfile.replace(".sentinel", "." + str(i) + ".log")

        statements.append('''Rscript %(tenx_dir)s/R/seurat_FindMarkers.R
                   --seuratobject=%(seurat_object)s
                   --seuratassay=RNA
                   --clusterids=%(cluster_ids)s
                   --cluster=%(i)s
                   --testuse=%(findmarkers_test)s
                   --minpct=%(findmarkers_minpct)s
                   --mindiffpct=-Inf
                   --maxcellsperident=%(findmarkers_maxcellsperident)s
                   --threshuse=%(findmarkers_threshuse)s
                   %(conserved_options)s
                   --annotation=annotation.dir/ensembl.to.entrez.tsv.gz
                   --numcores=%(findmarkers_numcores)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals()))

    P.run(statements)
    IOTools.touch_file(outfile)


@transform(cluster,
           regex(r"(.*)/cluster.sentinel"),
           r"\1/cluster.markers.dir/cluster.stats.sentinel")
def clusterStats(infile, outfile):
    '''
    Computation of per-cluster statistics

    This analysis is run in parallel for each cluster.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    if PARAMS["findmarkers_conserved"]:
        conservedfactor = PARAMS["findmarkers_conserved_factor"]
        conserved_options = '''--conservedfactor=%(conservedfactor)s
        '''
    else:
        conserved_options = ""

    statements = []

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"],
        cpu=1)

    for i in spec.clusters:

        if str(i) == "911":
            continue

        SPEC["log_file"] = outfile.replace(".sentinel", "." + str(i) + ".log")

        statements.append('''Rscript %(tenx_dir)s/R/seurat_clusterStats.R
                   --seuratobject=%(seurat_object)s
                   --seuratassay=RNA
                   --clusterids=%(cluster_ids)s
                   --cluster=%(i)s
                   %(conserved_options)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals()))

    P.run(statements)
    IOTools.touch_file(outfile)


@transform(clusterStats,
           regex(r"(.*)/cluster.stats.sentinel"),
           r"\1/summarise.stats.sentinel")
def summariseClusterStats(infile, outfile):
    '''
    Make summary tables of the cluster stats.

    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"],
        cpu=1)

    # make sumamary tables and plots of the differentially expressed genes
    statement = '''Rscript %(tenx_dir)s/R/summarise_clusterStats.R
                   --clusterids=%(cluster_ids)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@follows(summariseClusterStats)
@transform(findMarkers,
           regex(r"(.*)/findMarkers.sentinel"),
           r"\1/summariseMarkers.sentinel")
def summariseMarkers(infile, outfile):
    '''
    Make summary tables and plots of cluster marker genes.

    The per-cluster results files are aggregated.  Asummary excel
    file is generated. Tables for geneset enrichment testing
    are prepared.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    cluster_stats_table = os.path.join(spec.outdir,
                                       "cluster.stats.summary.table.tsv.gz")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"],
        cpu=1)

    # make sumamary tables and plots of the differentially expressed genes
    statement = '''Rscript %(tenx_dir)s/R/seurat_summariseMarkers.R
                   --seuratobject=%(seurat_object)s
                   --statstable=%(cluster_stats_table)s
                   --clusterids=%(cluster_ids)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)

    IOTools.touch_file(outfile)


@active_if(PARAMS["run_top_marker_heatmap"])
@transform(summariseMarkers,
           regex(r"(.*)/summariseMarkers.sentinel"),
           r"\1/topMarkerHeatmap.sentinel")
def topMarkerHeatmap(infile, outfile):
    '''
    Characterise cluster marker genes.

    Diagnostic summary plots of differentially expressed genes
    and violin plots of cluster marker gene expression are generated.

    Parallelised per-cluster.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    marker_table = os.path.join(os.path.dirname(infile),
                                "markers.summary.table.tsv.gz")


    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"],
        cpu=1)

    if PARAMS["plot_subgroup"] is not None:
        subgroup = '''--subgroup=%(plot_subgroup)s''' % PARAMS
    else:
        subgroup = ""

    statement = '''
    Rscript %(tenx_dir)s/R/seurat_topMarkerHeatmap.R
                   --seuratobject=%(seurat_object)s
                   --seuratassay=RNA
                   --slot=%(findmarkers_heatmap_slot)s
                   --markers=%(marker_table)s
                   --clusterids=%(cluster_ids)s
                   --pdf=%(plot_pdf)s
                   %(subgroup)s
                   --outdir=%(outdir)s
                    &> %(log_file)s
                    ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@active_if(PARAMS["run_characterise_markers"])
@transform(summariseMarkers,
           regex(r"(.*)/cluster.markers.dir/summariseMarkers.sentinel"),
           r"\1/cluster.marker.de.plots.dir/characteriseClusterMarkers.tex")
def characteriseClusterMarkers(infile, outfile):
    '''
    Characterise cluster marker genes.

    Diagnostic summary plots of differentially expressed genes
    and violin plots of cluster marker gene expression are generated.

    Parallelised per-cluster.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    marker_table = os.path.join(os.path.dirname(infile),
                                "markers.summary.table.tsv.gz")

    # not all clusters may have degenes
    degenes = pd.read_csv(marker_table, sep="\t")

    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"],
        cpu=1)

    statements = []
    tex = []

    for i in spec.clusters:

        if str(i) == "911":
            continue

        SPEC["log_file"] = outfile[:-len(".tex")] + "." + str(i) + ".log"

        statement = '''
                    Rscript %(tenx_dir)s/R/seurat_characteriseClusterDEGenes.R
                    --degenes=%(marker_table)s
                    --seuratobject=%(seurat_object)s
                    --seuratassay=RNA
                    --clusterids=%(cluster_ids)s
                    --cluster=%(i)s
                    --outdir=%(outdir)s
                    --useminfc=TRUE
                    --pointsize=%(plot_vpointsize)s
                    --ncol=%(plot_vncol)s
                    --nrow=%(plot_vnrow)s
                    --pdf=%(plot_pdf)s
                    --plotdirvar=clusterMarkerDEPlotsDir
                    &> %(log_file)s
                    ''' % dict(PARAMS, **SPEC, **locals())

        cluster_tex_file = ".".join(["characterise.degenes", str(i), "tex"])
        tex.append("\\input{\\clusterMarkerDEPlotsDir/" + cluster_tex_file + "}")
        statements.append(statement)

    P.run(statements)

    with open(outfile, "w") as out_tex:
        for line in tex:
            out_tex.write(line + "\n")


@active_if(PARAMS["run_extra_cluster_marker_plots"])
@transform(summariseMarkers,
           regex(r"(.*)/cluster.markers.dir/summariseMarkers.sentinel"),
           r"\1/cluster.marker.extra.plots.dir/cluster.marker.plots.sentinel")
def extraClusterMarkerPlots(infile, outfile):
    '''
       Make an additional set of per marker plots
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    marker_table = os.path.join(os.path.dirname(infile),
                                "markers.summary.table.tsv.gz")

    # not all clusters may have degenes
    markers = pd.read_csv(marker_table, sep="\t")
    clusters_with_markers = [x for x in markers.cluster.unique()]

    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"],
        cpu=1)

    statements = []

    # bring vars into local scope..
    rdims_vis_method = RDIMS_VIS_METHOD
    rdim1 = RDIMS_VIS_COMP_1
    rdim2 = RDIMS_VIS_COMP_2

    rdims_table = os.path.join(spec.component_dir,
                               "umap.dir",
                               "umap." + str(PARAMS["umap_mindist"]) + ".tsv.gz")

    if PARAMS["plot_subgroup"] is not None:
        group_opt = "--group=" + PARAMS["plot_subgroup"]
    else:
        group_opt = ""

    for i in clusters_with_markers:

        if str(i) == "911":
            continue

        SPEC["log_file"] = outfile[:-len(".sentinel")] + "." + str(i) + ".log"

        statement = '''
                    Rscript %(tenx_dir)s/R/seurat_cluster_marker_plots.R
                    --markers=%(marker_table)s
                    --seuratobject=%(seurat_object)s
                    --seuratassay=RNA
                    --clusterids=%(cluster_ids)s
                    --rdimstable=%(rdims_table)s
                    --rdim1=%(rdim1)s
                    --rdim2=%(rdim2)s
                    --cluster=%(i)s
                    --outdir=%(outdir)s
                    %(group_opt)s
                    --pdf=%(plot_pdf)s
                    &> %(log_file)s
                    ''' % dict(PARAMS, **SPEC, **locals())

        statements.append(statement)

    P.run(statements)

    IOTools.touch_file(outfile)



@transform(summariseMarkers,
           regex(r"(.*)/cluster.markers.dir/(.*).sentinel"),
           r"\1/cluster.marker.de.plots.dir/plotMarkerNumbers.sentinel")
def plotMarkerNumbers(infile, outfile):
    '''
    Summarise the numbers of per-cluster marker genes.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    marker_table = os.path.join(os.path.dirname(infile),
                                "markers.summary.table.tsv.gz")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_summariseMarkerNumbers.R
                   --degenes=%(marker_table)s
                   --clusterids=%(cluster_ids)s
                   --outdir=%(outdir)s
                   --minfc=2
                   --minpadj=0.05
                   --plotdirvar=clusterMarkerDEPlotsDir
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)



@active_if(PARAMS["run_exprsreport"])
@follows(summariseMarkers)
@transform(cluster,
           regex(r"(.*)/cluster.sentinel"),
           r"\1/cluster.marker.rdims.plots.dir/top.cluster.markers.sentinel")
def topClusterMarkers(infile, outfile):
    '''
    Identify the strongest cluster markers
    based on significance, expression frequency, expression level
    and fold change.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    marker_summary_file = os.path.join(spec.cluster_dir,
                                       "cluster.markers.dir",
                                       "markers.summary.table.tsv.gz")

    data = pd.read_csv(marker_summary_file, sep="\t")

    def _filterAndScore(data):
        # filter for strong cluster markers
        data = data[(data["p.adj"] < 0.01) &
                    (data["avg_logFC"].abs() > np.log(2)) &
                    (data["cluster_mean"] > 2) &
                    (data["pct.1"] > 0.25)]

        # compute a score based on all factors of interest.
        # here we use the product of the rank-normalised values
        # for fold change, expression level and adjusted p-value.
        # the aim is to give "better" markers higher scores.
        pscore = [1 - x for x in data["p.adj"].values]
        fscore = [np.exp(np.abs(x)) for x in data["avg_logFC"].values]
        escore = [np.log2(x) for x in data["cluster_mean"].values]

        # construct a matrix of the scores and take the geometric mean.
        temp = np.matrix([pscore,
                          fscore,
                          escore])

        data["score"] = np.squeeze(np.asarray(gmean(temp)))

        # keep only the best record for each gene
        # July 2020 - keep all so that we can see the combinations
        #             of markers for each cluster.
        # data = data.sort_values(["score"], ascending=False)
        # data = data.drop_duplicates(subset="gene_id", keep="first")

        # re-sort by cluster and then score
        data = data.sort_values(["cluster", "score"], ascending=[True, False])

        return data

    # define some helper functions..
    def _skimMarkers(data, n=40):
        # ensure we are ranked by cluster and score, best genes first.
        data = data.sort_values(["cluster", "score"], ascending=[True, False])

        # add the per cluster rankings
        data["grank"] = data.groupby(["cluster"]).cumcount()+1

        # de-duplicate keeping the marker for the cluster where
        # it has the best ranking.
        data = data.sort_values(["grank"], ascending=True)
        data = data.drop_duplicates(subset="gene_id", keep="first")

        # reorder and take the n best markers per cluster.
        data = data.sort_values(["cluster", "score"], ascending=[True, False])
        data = data.groupby("cluster").head(n)

        return data

    def _addGeneName(d):
        d["gene_name"] = d["gene"] + " (" + d["type"] + "; cluster " + \
                         d["cluster"].astype(str) + ")"
        return d

    def _write_tables(d, name="none"):

        # write the markers out to a table
        file_name = ".".join(["top", name, "cluster.markers.tsv"])
        markers_file = os.path.join(spec.outdir, file_name)
        d.to_csv(markers_file, header=True, sep="\t")

        log_name = ".".join(["plot.rdims.top", name, "cluster.markers.log"])
        SPEC["log_file"] = os.path.join(spec.outdir, log_name)

        if(d.shape[0] > 0):

            assay_file = markers_file.replace(".tsv",".rds")

            job_threads, job_memory, r_memory = TASK.get_resources(
                memory=PARAMS["resources_memory_standard"])


            statement = '''Rscript %(tenx_dir)s/R/seurat_get_assay_data.R
                           --seuratobject=%(seurat_object)s
                           --seuratassay=RNA
                           --slot=data
                           --features=%(markers_file)s
                           --outfile=%(assay_file)s
                           &> %(log_file)s
                       ''' % dict(PARAMS, **SPEC, **locals())
            return statement

        else:
            with(open(os.path.join(spec.outdir, "plot.rdims.genes.top." + name +
                                   ".cluster.markers.tex"), "w")) as tex:

                tex.write("No marker genes passed criteria for plotting\n")

            return False

    # keep up to n entries per cluster
    # note that groupby preserves the ordering.
    positive_markers = data[data["avg_logFC"] > 0]
    positive_markers = _filterAndScore(positive_markers)
    positive_markers = _skimMarkers(positive_markers,
                                    PARAMS["exprsreport_n_positive"])
    positive_markers["type"] = "+ve"
    positive_markers = _addGeneName(positive_markers)
    stat = _write_tables(positive_markers, "positive")

    statements = []
    if stat:
        statements.append(stat)

    negative_markers = data[data["avg_logFC"] < 0]
    negative_markers = _filterAndScore(negative_markers)
    negative_markers = _skimMarkers(negative_markers,
                                    PARAMS["exprsreport_n_negative"])
    negative_markers["type"] = "-ve"
    negative_markers = _addGeneName(negative_markers)
    stat = _write_tables(negative_markers, "negative")

    if stat:
        statements.append(stat)

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_high"])

    P.run(statements)

    IOTools.touch_file(outfile)



@active_if(PARAMS["run_exprsreport"])
@follows(summariseMarkers, RDIMS_VIS_TASK)
@transform(topClusterMarkers,
           regex(r"(.*)/top.cluster.markers.sentinel"),
           r"\1/plot.rdims.markers.sentinel")
def plotRdimsMarkers(infile, outfile):
    '''
    Visualise expression of discovered markers on rdims plots.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    def _plot(marker_table, assay_data, name):

        rdims_vis_method = RDIMS_VIS_METHOD
        rdim1 = RDIMS_VIS_COMP_1
        rdim2 = RDIMS_VIS_COMP_2
        rdims_table = os.path.join(spec.component_dir,
                                   "umap.dir",
                                   "umap." + str(PARAMS["umap_mindist"]) + ".tsv.gz")

        if PARAMS["plot_shape"] != "":
            shape = "--shapefactor=%(plot_shape)s" % PARAMS
        else:
            shape = ""

        statements = []
        markers = pd.read_csv(marker_table, sep="\t")

        name = "plot.rdims.genes.top." + name

        for cluster in markers.cluster.unique():

            if str(cluster) == "911":
                continue

            clog_file = spec.log_file.replace(".log",
                                              "." + name + "." + str(cluster) + ".log")

            statement = '''Rscript %(tenx_dir)s/R/plot_rdims_cluster_genes.R
                           --method=%(rdims_vis_method)s
                           --table=%(rdims_table)s
                           --assaydata=%(assay_data)s
                           --rdim1=%(rdim1)s
                           --rdim2=%(rdim2)s
                           %(shape)s
                           --name=%(name)s
                           --genetable=%(marker_table)s
                           --cluster=%(cluster)s
                           --pointsize=%(plot_pointsize)s
                           --pointalpha=%(plot_pointalpha)s
                           --pointpch=%(plot_pointpch)s
                           --maxcells=%(plot_maxcells)s
                           --outdir=%(outdir)s
                           --pdf=%(plot_pdf)s
                           &> %(clog_file)s
                       ''' % dict(PARAMS, **SPEC, **locals())

            statements.append(statement)

        return statements

    def _tex(marker_table, tex_file, name="markers_genes"):

        markers = pd.read_csv(marker_table, sep="\t")

        with open(tex_file,"w") as tex:

            for cluster in [x for x in markers.cluster.unique()]:

                if str(cluster) == "911":
                    continue

                npages = math.ceil(markers[
                    markers["cluster"]==cluster].shape[0] / 9)

                for page in list(range(1,int(npages)+1)):

                    page_title = "Cluster " + str(cluster)
                    page_title += ": " + name + " marker genes"

                    if page > 1:
                        page_title += " (continued)"

                    tex.write(templates.subsection % {"title": page_title})
                    tex.write("\n")

                    plot_name = "plot.rdims.genes.top." + name + ".cluster." +\
                                str(cluster) + ".page." + str(page)

                    plot_path = os.path.join(os.path.dirname(marker_table),
                                             plot_name)

                    heatmap_fig = {"width": "1", "height": "0.9",
                                   "path": plot_path,
                                   "caption": page_title
                                   }
                    tex.write(textwrap.dedent(
                        templates.figure % heatmap_fig))
                    tex.write("\n")


    positive_markers = os.path.join(spec.outdir,
                                    "top.positive.cluster.markers.tsv")
    positive_data = positive_markers.replace(".tsv", ".rds")
    pstats = _plot(positive_markers, positive_data, "positive")


    negative_markers = os.path.join(spec.outdir,
                                    "top.negative.cluster.markers.tsv")
    negative_data = negative_markers.replace(".tsv", ".rds")
    nstats = _plot(negative_markers, negative_data, "negative")


    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"])

    P.run( pstats + nstats )

    # write out the latex snippets
    positive_tex = os.path.join(spec.outdir,
                                "top.positive.cluster.markers.tex")
    _tex(positive_markers, positive_tex, "positive")

    negative_tex = os.path.join(spec.outdir,
                                "top.negative.cluster.markers.tex")
    _tex(negative_markers, negative_tex, "negative")



    IOTools.touch_file(outfile)



# ########################################################################### #
# ################# Within cluster between condition DE ##################### #
# ########################################################################### #

# Here genes differentially expressed between two conditions are identified
# at the cluster level.
#
# This analysis is optional.
#
# It is only run on samples prefixed with "all.", "agg." or "aligned."

@active_if(PARAMS["findmarkers_between"])
@follows(getGenesetAnnotations)
@transform(cluster,
           regex(r"(all.*|agg.*|aligned.*|integrated.*)/cluster.sentinel"),
           r"\1/condition.markers.dir/findMarkersBetweenConditions.sentinel")
def findMarkersBetweenConditions(infile, outfile):
    '''
    Identification of genes differentially expressed within-cluster.

    The two conditions to compare must be specified in the configuration file.

    This analysis is run in parallel for each cluster.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    statements = []

    if PARAMS["findmarkers_conserved_between"]:
        conservedfactor = PARAMS["findmarkers_conserved_between_factor"]
        conservedpadj = PARAMS["findmarkers_conserved_between_padj"]
        conserved_options = '''--conservedfactor=%(conservedfactor)s
            --conservedpadj=%(conservedpadj)s
        '''
    else:
        conserved_options = ""

    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"],
        cpu=PARAMS["findmarkers_numcores"])

    for i in spec.clusters:

        if str(i) == "911":
            continue

        cluster_log_file = outfile.replace(".sentinel", "." + str(i) + ".log")
        statements.append('''Rscript %(tenx_dir)s/R/seurat_FindMarkers.R
                   --seuratobject=%(seurat_object)s
                   --seuratassay=RNA
                   --clusterids=%(cluster_ids)s
                   --cluster=%(i)s
                   --testfactor=%(findmarkers_between_testfactor)s
                   --a=%(findmarkers_between_a)s
                   --b=%(findmarkers_between_b)s
                   --testuse=%(findmarkers_test)s
                   --threshuse=%(findmarkers_threshuse)s
                   --minpct=%(findmarkers_minpct)s
                   --mindiffpct=-Inf
                   --annotation=annotation.dir/ensembl.to.entrez.tsv.gz
                   %(conserved_options)s
                   --numcores=%(findmarkers_numcores)s
                   --outdir=%(outdir)s
                   &> %(cluster_log_file)s
                ''' % dict(PARAMS, **SPEC, **locals()))

    P.run(statements)
    IOTools.touch_file(outfile)


@active_if(PARAMS["findmarkers_between"])
@transform(findMarkersBetweenConditions,
           regex(r"(.*)/findMarkersBetweenConditions.sentinel"),
           r"\1/summariseMarkersBetweenConditions.sentinel")
def summariseMarkersBetweenConditions(infile, outfile):
    '''
    Make summary tables and plots of within-cluster DE genes.

    The per-cluster results files are aggregated. A heatmap of the top
    cluster markers is generated. A summary excel file is generated.
    Tables for geneset enrichment testing are prepared.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    testname = os.path.basename(outfile).split(".")[1]

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_standard"])

    # make sumamary tables and plots of the differentially expressed genes
    statement = '''Rscript %(tenx_dir)s/R/seurat_summariseMarkersBetween.R
                   --seuratobject=%(seurat_object)s
                   --seuratassay=RNA
                   --testfactor=%(findmarkers_between_testfactor)s
                   --a=%(findmarkers_between_a)s
                   --b=%(findmarkers_between_b)s
                   --clusterids=%(cluster_ids)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


@active_if(PARAMS["findmarkers_between"])
@transform(summariseMarkersBetweenConditions,
           regex(r"(.*)/condition.markers.dir/summariseMarkersBetweenConditions.sentinel"),
           r"\1/condition.marker.de.plots.dir/characteriseClusterMarkersBetween.tex")
def characteriseClusterMarkersBetweenConditions(infile, outfile):
    '''
    Characterise within-cluster DE genes.

    Diagnostic summary plots of differentially expressed genes
    and violin plots are generated.

    Parallelised per-cluster.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    marker_table = os.path.join(os.path.dirname(infile),
                                "markers.between." +
                                PARAMS["findmarkers_between_testfactor"] +
                                ".summary.table.tsv.gz")

    # not all clusters may have degenes
    degenes = pd.read_csv(marker_table, sep="\t")
    declusters = [x for x in set(degenes["cluster"].values)]

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"])

    statements = []
    tex = []

    for i in declusters:

        if str(i) == "911":
            continue

        cluster_log_file = outfile[:-len(".tex")] + "." + str(i) + ".log"

        statement = '''
                    Rscript %(tenx_dir)s/R/seurat_characteriseClusterDEGenes.R
                    --degenes=%(marker_table)s
                    --seuratobject=%(seurat_object)s
                    --seuratassay=RNA
                    --clusterids=%(cluster_ids)s
                    --cluster=%(i)s
                    --testfactor=%(findmarkers_between_testfactor)s
                    --a=%(findmarkers_between_a)s
                    --b=%(findmarkers_between_b)s
                    --useminfc=FALSE
                    --outdir=%(outdir)s
                    --plotdirvar=conditionMarkerDEPlotsDir
                    &> %(cluster_log_file)s
                    ''' % dict(PARAMS, **SPEC, **locals())

        cluster_tex_file = ".".join(["characterise.degenes", str(i),
                                     "between.tex"])

        tex.append("\\input{\\conditionMarkerDEPlotsDir/" + cluster_tex_file + "}")
        statements.append(statement)

    P.run(statements)

    with open(outfile, "w") as out_tex:
        for line in tex:
            out_tex.write(line + "\n")


@active_if(PARAMS["findmarkers_between"])
@transform(summariseMarkersBetweenConditions,
           regex(r"(.*)/condition.markers.dir/(.*).sentinel"),
           r"\1/condition.marker.de.plots.dir/plotMarkerNumbersBetween.sentinel")
def plotMarkerNumbersBetweenConditions(infile, outfile):
    '''
    Summarise the numbers of within-cluster DE genes.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    marker_table = os.path.join(os.path.dirname(infile),
                                "markers.between." +
                                PARAMS["findmarkers_between_testfactor"] +
                                ".summary.table.tsv.gz")

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_low"])

    statement = '''Rscript %(tenx_dir)s/R/seurat_summariseMarkerNumbers.R
                   --degenes=%(marker_table)s
                   --clusterids=%(cluster_ids)s
                   --testfactor=%(findmarkers_between_testfactor)s
                   --a=%(findmarkers_between_a)s
                   --b=%(findmarkers_between_b)s
                   --minfc=2
                   --minpadj=0.05
                   --outdir=%(outdir)s
                   --plotdirvar=conditionMarkerDEPlotsDir
                   &> %(log_file)s
                ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


# ########################################################################### #
# ########### marker gene (and within cluster DE) analysis ################## #
# ########################################################################### #

@follows(characteriseClusterMarkers,
         topMarkerHeatmap,
         plotMarkerNumbers,
         characteriseClusterMarkersBetweenConditions,
         plotMarkerNumbersBetweenConditions)
@files(None, "markers.sentinel")
def markers(infile, outfile):
    '''
       Collect the marker plots
    '''

    IOTools.touch_file(outfile)


# ########################################################################### #
# ######################### Geneset Analysis ################################ #
# ########################################################################### #

def parseGMTs(param_keys=["gmt_pathway_files_"]):
    '''Helper function for parsing the lists of GMT files'''

    all_files = []
    all_names = []

    for param_key in param_keys:


        gmts = [x for x in PARAMS.keys()
                if x.startswith(param_key)]

        if len(gmts) > 0:
            all_files += [PARAMS[x] for x in gmts]

            all_names += [x.replace(param_key, "")
                              for x in gmts]

    if len(all_files) == 0:
        all_files = "none"
        all_names = "none"
    else:
        all_files = ",".join(all_files)
        all_names = ",".join(all_names)

    return all_names, all_files


# ------------------- < between cluster geneset analysis > ------------------ #

@active_if(PARAMS["run_genesets"])
@follows(summariseMarkers)
@transform(findMarkers,
           regex(r"(.*)/cluster.markers.dir/.*.sentinel"),
           add_inputs(getGenesetAnnotations),
           r"\1/cluster.genesets.dir/geneset.analysis.sentinel")
def genesetAnalysis(infiles, outfile):
    '''
    Naive geneset over-enrichment analysis of cluster marker genes.

    Testing is performed with the gsfisher package.

    GO categories and KEGG pathways are tested by default.

    Arbitrary sets of genes cat be supplied as GMT files
    (e.g. such as those from MSigDB).
    '''

    findMarkersLog, genesetAnno = infiles

    spec, SPEC = TASK.get_vars(findMarkersLog, outfile, PARAMS)

    anno = os.path.join(os.path.dirname(genesetAnno),
                        "ensembl.to.entrez.tsv.gz")

    kegg_pathways = os.path.join(os.path.dirname(genesetAnno),
                                 "kegg_pathways.rds")

    param_keys = ["gmt_celltype_files_",
                  "gmt_pathway_files_"]
    gmt_names, gmt_files = parseGMTs(param_keys=param_keys)

    statements = []

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"])

    for i in spec.clusters:

        if str(i) == "911":
            continue

        cluster_log_file = os.path.join(spec.outdir, "geneset.analysis." + str(i) + ".log")

        markers = os.path.join(spec.indir, "markers.summary.table.tsv.gz")

        universe = os.path.join(
            spec.indir, "markers.cluster." + str(i) + ".universe.tsv.gz")

        if not os.path.exists(universe):
            E.warn("Skipping geneset analysis: %s does not exist" % universe)
            continue

        statements.append('''Rscript %(tenx_dir)s/R/genesetAnalysis.R
                            --markers=%(markers)s
                            --universe=%(universe)s
                            --species=%(annotation_species)s
                            --annotation=%(anno)s
                            --kegg_pathways=%(kegg_pathways)s
                            --gmt_names=%(gmt_names)s
                            --gmt_files=%(gmt_files)s
                            --cluster=%(i)s
                            --adjpthreshold=%(genesets_marker_adjpthreshold)s
                            --direction=positive
                            --outdir=%(outdir)s
                            &> %(cluster_log_file)s
                      ''' % dict(PARAMS, **SPEC, **locals()))

    P.run(statements)
    IOTools.touch_file(outfile)


@active_if(PARAMS["run_genesets"])
@transform(genesetAnalysis,
           regex(r"(.*)/.*.sentinel"),
           r"\1/summarise.geneset.analysis.sentinel")
def summariseGenesetAnalysis(infile, outfile):
    '''
    Summarise the geneset over-enrichment analyses of cluster marker genes.

    Enriched pathways are summarised in an Excel table and a heatmap.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    # need to sort out the dependencies properly!
    genesetdir = os.path.dirname(infile)

    param_keys = ["gmt_celltype_files_",
                  "gmt_pathway_files_"]
    gmt_names, gmt_files = parseGMTs(param_keys=param_keys)

    # Read clusters

    use_adjusted = str(PARAMS["genesets_use_adjusted_pvalues"]).upper()
    show_common = str(PARAMS["genesets_show_common"]).upper()

    show_detailed = str(PARAMS["genesets_show_detailed"])

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"])

    statement = '''Rscript %(tenx_dir)s/R/summariseGenesets.R
                         --genesetdir=%(genesetdir)s
                         --gmt_names=%(gmt_names)s
                         --show_detailed=%(show_detailed)s
                         --clusters=%(cluster_table)s
                         --mingenes=%(genesets_min_fg_genes)s
                         --pvaluethreshold=%(genesets_pvalue_threshold)s
                         --padjustmethod=%(genesets_padjust_method)s
                         --useadjusted=%(use_adjusted)s
                         --minoddsratio=%(genesets_min_odds_ratio)s
                         --showcommon=%(show_common)s
                         --outprefix=%(outdir)s/cluster.genesets
                         --prefix=genesets
                         --plotdirvar=clusterGenesetsDir
                         --pdf=%(plot_pdf)s
                    &> %(log_file)s
                      ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


# ------------------- < within cluster geneset analysis > ------------------- #

@active_if(PARAMS["run_genesets"])
@active_if(PARAMS["findmarkers_between"])
@follows(summariseMarkersBetweenConditions)
@transform(findMarkersBetweenConditions,
           regex(r"(.*)/condition.markers.dir/.*.sentinel"),
           add_inputs(getGenesetAnnotations),
           r"\1/condition.genesets.dir/geneset.analysis.between.conditions.sentinel")
def genesetAnalysisBetweenConditions(infiles, outfile):
    '''
    Naive geneset over-enrichment analysis of genes DE within-cluster.

    Testing is performed with the gsfisher package.

    GO categories and KEGG pathways are tested by default.

    Arbitrary sets of genes cat be supplied as GMT files
    (e.g. such as those from MSigDB).
    '''

    findMarkersLog, genesetAnno = infiles

    spec, SPEC = TASK.get_vars(findMarkersLog, outfile, PARAMS)

    anno = os.path.join(os.path.dirname(genesetAnno),
                        "ensembl.to.entrez.tsv.gz")

    kegg_pathways = os.path.join(os.path.dirname(genesetAnno),
                                 "kegg_pathways.rds")

    gmt_names, gmt_files = parseGMTs(param_keys=["gmt_pathway_files_"])

    statements = []

    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"])

    for i in spec.clusters:

        if str(i) == "911":
            continue

        cluster_log_file = os.path.join(
            spec.outdir, "geneset.analysis.between." + str(i) + ".log")

        markers = os.path.join(
            spec.indir, "markers.between." +
            PARAMS["findmarkers_between_testfactor"] +
            ".summary.table.tsv.gz")

        universe = os.path.join(
            spec.indir, "markers.between." +
            PARAMS["findmarkers_between_testfactor"] +
            ".cluster." + str(i) + ".universe.tsv.gz")

        if not os.path.exists(universe):
            E.warn("Skipping geneset analysis: %s does not exist" % universe)
            continue

        statements.append('''Rscript %(tenx_dir)s/R/genesetAnalysis.R
                            --markers=%(markers)s
                            --universe=%(universe)s
                            --species=%(annotation_species)s
                            --annotation=%(anno)s
                            --kegg_pathways=%(kegg_pathways)s
                            --gmt_names=%(gmt_names)s
                            --gmt_files=%(gmt_files)s
                            --cluster=%(i)s
                            --adjpthreshold=%(genesets_marker_adjpthreshold)s
                            --direction=both
                            --prefix=genesets.between
                            --outdir=%(outdir)s
                            &> %(cluster_log_file)s
                      ''' % dict(PARAMS, **SPEC, **locals()))

    P.run(statements)
    IOTools.touch_file(outfile)


@active_if(PARAMS["run_genesets"])
@active_if(PARAMS["findmarkers_between"])
@transform(genesetAnalysisBetweenConditions,
           regex(r"(.*)/.*.sentinel"),
           r"\1/summarise.geneset.analysis.between.conditions.sentinel")
def summariseGenesetAnalysisBetweenConditions(infile, outfile):
    '''
    Summarise the geneset over-enrichment analyses of genes DE within-cluster.

    Enriched pathways are summarised in an Excel table and a heatmap.
    '''

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    genesetdir = os.path.dirname(infile)

    gmt_names, gmt_files = parseGMTs(param_keys=["gmt_pathway_files_"])

    use_adjusted = str(PARAMS["genesets_use_adjusted_pvalues"]).upper()
    show_common = str(PARAMS["genesets_show_common"]).upper()
    show_detailed = str(PARAMS["genesets_show_detailed"])


    # set the job threads and memory
    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"])

    statement = '''Rscript %(tenx_dir)s/R/summariseGenesets.R
                         --genesetdir=%(genesetdir)s
                         --gmt_names=%(gmt_names)s
                         --show_detailed=%(show_detailed)s
                         --clusters=%(cluster_table)s
                         --mingenes=%(genesets_min_fg_genes)s
                         --pvaluethreshold=%(genesets_pvalue_threshold)s
                         --padjustmethod=%(genesets_padjust_method)s
                         --minoddsratio=%(genesets_min_odds_ratio)s
                         --useadjusted=%(use_adjusted)s
                         --showcommon=%(show_common)s
                         --outprefix=%(outdir)s/condition.genesets
                         --prefix=genesets.between
                         --plotdirvar=conditionGenesetsDir
                         --pdf=%(plot_pdf)s
                    &> %(log_file)s
                      ''' % dict(PARAMS, **SPEC, **locals())

    P.run(statement)
    IOTools.touch_file(outfile)


# ---------------------- < geneset analysis target > ---------------------- #

@files([summariseGenesetAnalysis,
        summariseGenesetAnalysisBetweenConditions],
       "genesets.sentinel")
def genesets(infile, outfile):
    '''
       Intermediate target to run geneset tasks
    '''

    IOTools.touch_file(outfile)


# ########################################################################### #
# ##################### Target to collect plots ############################# #
# ########################################################################### #

@files([exploreHvgAndCellCycle,
         summariseSingleR,
         compareClusters,
         clustree,
         paga,
#         plotTSNEPerplexities,
         plotRdimsFactors,
         plotRdimsClusters,
         plotRdimsGenes,
         plotRdimsMarkers,
         plotDiffusionMap,
         phate,
         plotGroupNumbers,
         scvelo,
         extraClusterMarkerPlots,
         knownMarkerViolins], "plots.sentinel")
def plots(infile, outfile):
    '''
    Intermediate target to collect plots.
    '''

    IOTools.touch_file(outfile)


# ########################################################################### #
# ################## PDF report generation (via Latex) ###################### #
# ########################################################################### #

# High quality pdf reports are generated which can be easily shared.
#
# The reports incorporate raster (png) graphics. PDF versions of each graphic
# are also avaliable in the individual run folders.

# @follows(taskSummary,
#          markers,
#          genesets,
#          plots,
#          summariseGenesetAnalysis)

# here we use add_inputs to force the pipeline to
# re-run the task if it is behind any of the upstream.

@transform(plotRdimsClusters,
           regex("(.*)/rdims.visualisation.dir/plot.rdims.cluster.sentinel"),
           add_inputs(taskSummary, markers, genesets, plots, summariseGenesetAnalysis),
           r"\1/latex.dir/report.vars.sty")
def latexVars(infiles, outfile):
    '''
    Prepare a file containing the latex variable definitions.
    '''

    infile = infiles[0]

    spec, SPEC = TASK.get_vars(infile, outfile, PARAMS)

    outdir = spec.outdir



    clusterDir = spec.cluster_dir

    compDir = spec.component_dir

    outfile_name = os.path.basename(outfile)

    singleRDir = os.path.join(compDir,
                              "singleR.dir")

    clusterGenesetsDir = os.path.join(clusterDir,
                              "cluster.genesets.dir")
    clusterMarkerDEPlotsDir = os.path.join(clusterDir,
                              "cluster.marker.de.plots.dir")
    clusterMarkerRdimsPlotsDir = os.path.join(clusterDir,
                                             "cluster.marker.rdims" +\
                                             ".plots.dir")
    clusterMarkersDir = os.path.join(clusterDir,
                                     "cluster.markers.dir")
    conditionGenesetsDir = os.path.join(clusterDir,
                              "condition.genesets.dir")
    conditionMarkerDEPlotsDir = os.path.join(clusterDir,
                              "condition.marker.de.plots.dir")
    conditionMarkersDir = os.path.join(clusterDir,
                                     "condition.markers.dir")
    genelistsDir = os.path.join(clusterDir,
                                "genelists.dir")
    knownmarkersDir = os.path.join(clusterDir,
                                   "known.markers.dir")
    diffmapDir = os.path.join(clusterDir,
                              "dm.visualisation.dir")
    groupNumbersDir = os.path.join(clusterDir,
                                   "group.numbers.dir")
    umapDir = os.path.join(compDir,
                           "umap.dir")

    rdimsVisClusterDir = os.path.join(clusterDir,
                               "rdims.visualisation.dir")
    rdimsVisFactorDir = os.path.join(compDir,
                               "rdims.visualisation.dir")

    rdimsVisSingleRDir = os.path.join(compDir, "singleR.dir",
                               "rdims.visualisation.dir")

    # rdimsVisMethod = RDIMS_VIS_METHOD
    rdimsVisMethodShort = "umap"
    rdimsVisMethod = "umap.mindist_" + str(PARAMS["umap_mindist"])

    velocityDir = os.path.join(compDir,
                               "velocity.dir")
    pagaDir = os.path.join(clusterDir,
                               "paga.dir")
    phateDir = os.path.join(compDir,
                            "phate.dir")
    sampleDir = spec.sample_dir

    clusterDirBaseName = os.path.basename(clusterDir)

    nPCs = spec.components
    resolution = spec.resolution
    deTest = PARAMS["findmarkers_test"]
    algorithm = PARAMS["cluster_algorithm"]


    runName = nPCs + "\\_" + resolution #clusterDirBaseName.replace("_", "\\_")

    runDetails = ("no. components: " + str(nPCs) +
                  ", cluster resolution: " + str(resolution) +
                  ", cluster algorithm: " + str(algorithm) +
                  ", de test: " + deTest)

    reductionType = PARAMS["dimreduction_method"]

    sample = Path(outfile).parts[0].split(".")[0]

    jobName = runName  #sample + "_" + runName.replace(".cluster.dir", "")

    sample = sample.replace("_", "\\_")

    latentvars = PARAMS["regress_latentvars"].replace("_", "\\_")

    if PARAMS["findmarkers_conserved"]:
        conservedFactor = PARAMS["findmarkers_conserved_factor"]
        conservedFactor = conservedFactor.replace("_", "\\_")
    else:
        conservedFactor = "None"

    if PARAMS["findmarkers_conserved_between"]:
        conservedBetweenFactor = PARAMS["findmarkers_conserved_between_factor"]
        conservedBetweenFactor = conservedBetweenFactor.replace("_", "\\_")
    else:
        conservedBetweenFactor = "None"

    if PARAMS["normalization_method"] == "log-normalization":
        varGeneMethod = PARAMS["vargenes_method"]
    elif PARAMS["normalization_method"] == "sctransform":
        varGeneMethod = "SCTransform"
    else:
        raise ValueError("unrecognised normalization method")

    vars = {"sample": "%(sample)s" % locals(),
            "projectName": "%(projectname)s" % PARAMS,
            "reportAuthor": "%(author)s" % PARAMS,
#            "runDir": "%(runDir)s" % locals(),
            "compDir": "%(compDir)s" % locals(),
            "sampleDir": "%(sampleDir)s" % locals(),
            "clusterDir": "%(clusterDir)s" % locals(),
            "singleRDir": "%(singleRDir)s" % locals(),
#            "tsneDir": "%(tsneDir)s" % locals(),
            "clusterGenesetsDir": "%(clusterGenesetsDir)s" % locals(),
            "clusterMarkerDEPlotsDir": "%(clusterMarkerDEPlotsDir)s" % locals(),
            "clusterMarkerRdimsPlotsDir": "%(clusterMarkerRdimsPlotsDir)s" % locals(),
            "clusterMarkersDir": "%(clusterMarkersDir)s" % locals(),
            "conditionGenesetsDir": "%(conditionGenesetsDir)s" % locals(),
            "conditionMarkerDEPlotsDir": "%(conditionMarkerDEPlotsDir)s" % locals(),
            "conditionMarkersDir": "%(conditionMarkersDir)s" % locals(),
            "knownmarkersDir": "%(knownmarkersDir)s" % locals(),
            "genelistsDir": "%(genelistsDir)s" % locals(),
            "diffmapDir": "%(diffmapDir)s" % locals(),
            "groupNumbersDir": "%(groupNumbersDir)s" % locals(),
            "umapDir": "%(umapDir)s" % locals(),
            "rdimsVisClusterDir": "%(rdimsVisClusterDir)s" % locals(),
            "rdimsVisFactorDir": "%(rdimsVisFactorDir)s" % locals(),
            "rdimsVisSingleRDir": "%(rdimsVisSingleRDir)s" % locals(),
            "rdimsVisMethod": "%(rdimsVisMethod)s" % locals() ,
            "rdimsVisMethodShort": "%(rdimsVisMethodShort)s" % locals() ,
            "velocityDir": "%(velocityDir)s" % locals(),
            "pagaDir": "%(pagaDir)s" % locals(),
            "phateDir": "%(phateDir)s" % locals(),
            "runName": "%(runName)s" % locals(),
            "runDetails": "%(runDetails)s" % locals(),
            "tenxDir": "%(tenx_dir)s" % PARAMS,
            "nPCs": "%(nPCs)s" % locals(),
            "normalizationMethod": "%(normalization_method)s" % PARAMS,
            "reductionType": "%(reductionType)s" % locals(),
#            "tSNEPerplexity": "%(tsne_perplexity)s" % PARAMS,
#            "tSNEMaxIter": "%(tsne_maxiter)s" % PARAMS,
#            "tSNEFast": "%(tsne_fast)s" % PARAMS,
            "nPositiveMarkers": "%(exprsreport_n_positive)s" % PARAMS,
            "nNegativeMarkers": "%(exprsreport_n_negative)s" % PARAMS,
            "nnK": "%(neighbors_n_neighbors)s" % PARAMS,
            "nnMethod": "%(neighbors_method)s" % PARAMS,
            "nnMetric": "%(neighbors_metric)s" % PARAMS,
            "resolution": "%(resolution)s" % locals(),
            "clusteringAlgorithm": "%(algorithm)s" % locals(),
            "deTest": "%(deTest)s" % locals(),
            "threshUse": "%(findmarkers_threshuse)s" % PARAMS,
            "minPct": "%(findmarkers_minpct)s" % PARAMS,
            "qcMinGenes": "%(qc_mingenes)s" % PARAMS,
            "qcMaxMito": "%(qc_maxpercentmito)s" % PARAMS,
            "minCells": "%(qc_mincells)s" % PARAMS,
            "modelType": "%(regress_modeluse)s" % PARAMS,
            "latentVariables": "%(latentvars)s" % locals(),
            "cellCycle": "%(regress_cellcycle)s" % PARAMS,
            "varGeneMethod": "%(varGeneMethod)s" % locals(),
            "sdCutOff": "%(vargenes_sdcutoff)s" % PARAMS,
            "conservedFactor": "%(conservedFactor)s" % locals(),
            "conservedBetweenFactor": "%(conservedBetweenFactor)s" % locals()}

    with open(outfile, "w") as ofh:
        for command, value in vars.items():

            ofh.write("\\newcommand{\\" + command + "}{" + value + "}\n")


@active_if(PARAMS["run_exprsreport"])
@transform(latexVars,
           regex("(.*)/report.vars.sty"),
           r"\1/geneExpressionReport.pdf")
def geneExpressionReport(infile, outfile):
    '''
     Prepare a PDF report of the expression of genes interest.

     The expression of  manually specified sets of genes and of
     discovered cluster markers is visualised.
    '''

    outfile_name = os.path.basename(outfile)
    jobName = outfile_name[:-len(".pdf")]

    outdir = os.path.dirname(outfile)
    compilation_dir = os.path.join(outdir, ".latex_compilation.dir")

    latexVars = os.path.join(outdir, "report.vars")

    try:
        os.stat(compilation_dir)
    except FileNotFoundError:
        os.mkdir(compilation_dir)

    statement = '''pdflatex -output-directory=%(compilation_dir)s
                            -jobname=%(jobName)s
                            %(draft_mode)s
      '\\def\\latexVars{%(latexVars)s}
       \\def\\reportTitle{pipeline\\_seurat.py: gene expression report}
       \\input %(tenx_dir)s/pipelines/pipeline_scxl/geneExpressionReport.tex
       \\input %(tenx_dir)s/latex/endmatter.tex'
       '''

    # Deliberately run twice - necessary for LaTeX compilation..
    draft_mode="-draftmode"
    P.run(statement)
    draft_mode=""
    P.run(statement)

    # Move the compiled pdfs to report.dir
    shutil.move(os.path.join(compilation_dir, outfile_name),
                outfile)


@follows(summariseMarkers,
         extraClusterMarkerPlots,
         characteriseClusterMarkers)
@active_if(PARAMS["run_marker_report"])
@transform(latexVars,
           regex("(.*)/report.vars.sty"),
           r"\1/clusterMarkerReport.pdf")
def markerReport(infile, outfile):
    '''
     Prepare a PDF report visualising the discovered cluster markers
    '''

    spec, SPEC = TASK.get_vars(infile, infile, PARAMS)

    marker_table = os.path.join(spec.cluster_dir,
                                "cluster.markers.dir",
                                "markers.summary.table.tsv.gz")

    # not all clusters may have degenes
    markers = pd.read_csv(marker_table, sep="\t")
    clusters_with_markers = [x for x in markers.cluster.unique()]

    tex_file = os.path.join(spec.outdir,
                            "cluster.marker.report.tex")

    with open(tex_file,"w") as tex:

        def _add_figure(plot_file=None,
                        caption=None,
                        tex=tex,
                        width="1",
                        height="0.9"):

            heatmap_fig = {"width": "1", "height": "0.9",
                           "path": plot_file,
                           "caption": caption
            }

            tex.write(textwrap.dedent(
                templates.figure % heatmap_fig))
            tex.write("\n")

        tex.write(templates.subsection % {"title": "overview plots"})
        tex.write("\n")

        _add_figure(os.path.join(spec.cluster_dir, "rdims.visualisation.dir",
                                 "umap.mindist_" + str(PARAMS["umap_mindist"]) +\
                                 ".cluster_id"),
                    width = "1", height= ".9",
                    caption = "UMAP coloured by cluster  (resolution " + spec.resolution + ")")


        tmh = os.path.join(spec.cluster_dir, "cluster.markers.dir",
                           "markers.summary.heatmap")

        if(os.path.exists(tmh)):
            _add_figure(tmh,
                        width = "1", height= "0.9",
                        caption = "Marker summary heatmap (resolution " + spec.resolution + ")")


        for clust in clusters_with_markers:

            if str(clust) == "911":
                continue

            tex.write(templates.subsection % {"title": "Markers for cluster: " + str(clust)})
            tex.write("\n")

            fig = "heatmap"
            _add_figure(os.path.join(spec.cluster_dir, "cluster.marker.extra.plots.dir",
                                     "cluster." + str(clust) + "." + fig),
                        width = "1", height= "0.25",
                        caption = "cluster " + str(clust) + " " + fig)

            fig = "dotplot"
            _add_figure(os.path.join(spec.cluster_dir, "cluster.marker.extra.plots.dir",
                                     "cluster." + str(clust) + "." + fig),
                        width = "1", height= "0.3",
                        caption = "cluster " + str(clust) + " " +fig)

            fig = "rdims"
            _add_figure(os.path.join(spec.cluster_dir, "cluster.marker.extra.plots.dir",
                                     "cluster." + str(clust) + "." + fig),
                        width = "1", height= "0.5",
                        caption = "cluster " + str(clust) + " expression dot plot")

            # add the scatter plots.
            _add_figure(os.path.join(spec.cluster_dir,
                                     "cluster.marker.de.plots.dir",
                                     "dePlots." + str(clust)),
                        width = "1", height= "0.9",
                        caption = "cluster " + str(clust) + " differential expression plots")

            # Add the violin plots

            fig = "violins"
            _add_figure(os.path.join(spec.cluster_dir, "cluster.marker.extra.plots.dir",
                                     "cluster." + str(clust) + "." + fig),
                        width = "1", height= "0.7",
                        caption = "cluster " + str(clust) + " " + fig)




    outfile_name = os.path.basename(outfile)
    jobName = outfile_name[:-len(".pdf")]

    compilation_dir = os.path.join(spec.outdir, ".latex_compilation.dir")

    latexVars = os.path.join(spec.outdir, "report.vars")

    try:
        os.stat(compilation_dir)
    except FileNotFoundError:
        os.mkdir(compilation_dir)

    job_threads, job_memory, r_memory = TASK.get_resources(
        memory=PARAMS["resources_memory_min"])


    statement = '''pdflatex -output-directory=%(compilation_dir)s
                            -jobname=%(jobName)s
                            %(draft_mode)s
      '\\def\\latexVars{%(latexVars)s}
       \\def\\reportTitle{pipeline\\_seurat.py: cluster marker report}
       \\input %(tenx_dir)s/pipelines/pipeline_scxl/clusterMarkerReport.tex
       \\input %(tex_file)s
       \\input %(tenx_dir)s/latex/endmatter.tex'
       '''

    print(statement)

    # Deliberately run twice - necessary for LaTeX compilation..
    draft_mode = "-draftmode"
    P.run(statement % dict(PARAMS, **SPEC, **locals()))

    draft_mode = ""
    P.run(statement % dict(PARAMS, **SPEC, **locals()))

    # Move the compiled pdfs to report.dir
    shutil.move(os.path.join(compilation_dir, outfile_name),outfile)


@transform(latexVars,
           regex("(.*)/report.vars.sty"),
           r"\1/summaryReport.pdf")
def summaryReport(infile, outfile):
    '''
    Prepare a PDF summary report.
    '''

    outfile_name = os.path.basename(outfile)
    jobName = outfile_name[:-len(".pdf")]

    outdir = os.path.dirname(outfile)
    rundir = Path(outdir).parents[0]

    compilation_dir = os.path.join(outdir, ".latex_compilation.dir")

    latexVars = os.path.join(outdir, "report.vars.sty")

    try:
        shutil.rmtree(compilation_dir)
    except FileNotFoundError:
        pass

    os.mkdir(compilation_dir)

    # get the latex variables
    statement = '''pdflatex -output-directory=%(compilation_dir)s
                            -jobname=%(jobName)s
                            %(draft_mode)s
      '\\input %(latexVars)s
       \\def\\reportTitle{pipeline\\_seurat.py: summary report}
                '''
    # get the intro
    statement += '''
      \\input %(tenx_dir)s/pipelines/pipeline_scxl/introReport.tex
      '''

    # begin the report (qc, hvg, pca dimension reduction)
    if(os.path.exists("data.dir")):
        statement += '''
          \\input %(tenx_dir)s/pipelines/pipeline_scxl/beginReport.tex
          '''
        if PARAMS["run_explore_hvg_and_cell_cycle"]:
            statement += '''
            \\input %(tenx_dir)s/pipelines/pipeline_scxl/hvgAndCellCycle.tex
            '''

        if PARAMS["normalization_method"] != "sctransform" and +\
           PARAMS["run_jackstraw"] == True:
            statement += '''
            \\input %(tenx_dir)s/pipelines/pipeline_scxl/jackstrawSection.tex
            '''



    # add the section to visualise clusters and factors in reduced dimensions
    # (plots made by tsne or umap)
    statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/rdimsVisSection.tex
        '''

    # singleR section
    if(PARAMS["run_singleR"]):
        statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/singleRSection.tex
        '''

    # add the section with plots of cell and gene numbers etc.
    statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/numbersSection.tex
        '''

    if(PARAMS["run_compare_clusters"]):
       statement += '''
       \\input %(tenx_dir)s/pipelines/pipeline_scxl/clusteringSection.tex
       '''

    nresolutions = len(str(PARAMS["runspecs_cluster_resolutions"]).split(","))

    if(nresolutions > 1):
       statement += '''
       \\input %(tenx_dir)s/pipelines/pipeline_scxl/clustree.tex
       '''


    if(PARAMS["run_diffusionmap"]):
        statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/diffusionSection.tex
        '''

    if(PARAMS["run_phate"]):
        statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/phateSection.tex
        '''

    if(PARAMS["run_paga"]):
        statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/pagaSection.tex
        '''

    if(PARAMS["run_velocity"]):
        statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/scveloSection.tex
        '''

    if(PARAMS["run_velocity"] and PARAMS["run_paga"]):
        statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/scveloForceDirectedGraphSection.tex
        '''

    if(PARAMS["run_velocity"] and PARAMS["run_phate"]):
        statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/scveloPhateSection.tex
        '''

    if(PARAMS["run_knownmarkers"]):
        statement += '''
         \\input %(tenx_dir)s/pipelines/pipeline_scxl/knownmarkersSection.tex
        '''

    statement += '''
      \\input %(tenx_dir)s/pipelines/pipeline_scxl/markerGenes.tex
      '''

    if(PARAMS["run_top_marker_heatmap"]):
        statement += '''
        \\input %(tenx_dir)s/pipelines/pipeline_scxl/topMarkerHeatmap.tex
        '''

    if(PARAMS["run_characterise_markers"]): # and not ...
        statement += '''
        \\input %(tenx_dir)s/pipelines/pipeline_scxl/markerGenesByCluster.tex
        '''

    if(PARAMS["run_genesets"]):
        statement += '''
        \\input %(tenx_dir)s/pipelines/pipeline_scxl/genesetSection.tex
                     '''

    # When relevant, add section that compares
    # two conditions within each cluster
    if os.path.exists(
            os.path.join(rundir, "condition.markers.dir", "findMarkersBetweenConditions.sentinel")):

        wcc_section_name = "withinClusterComparisonSection.tex"
        statement += '''
          \\input %(tenx_dir)s/pipelines/pipeline_scxl/%(wcc_section_name)s
          '''

        if(PARAMS["run_genesets"]):
            statement += '''
        \\input %(tenx_dir)s/pipelines/pipeline_scxl/genesetBetweenSection.tex
                         '''


    statement += '''\\input %(tenx_dir)s/latex/endmatter.tex'
    '''

    # Deliberately run twice - necessary for LaTeX compilation..
    draft_mode = "-draftmode"
    P.run(statement)

    draft_mode = ""
    P.run(statement)

    # Move the compiled pdfs to report.dir
    shutil.move(os.path.join(compilation_dir, outfile_name),
                outfile)


@follows(mkdir("reports.dir"), geneExpressionReport, markerReport)
@transform(summaryReport,
           regex(r"(.*).seurat.dir/components.(.*).dir/cluster.(.*).dir/latex.dir/summaryReport.pdf"),
           r"reports.dir/\1.\2_\3/export.sentinel")
def export(infile, outfile):
    '''
    Link output files to a directory in the "reports.dir" folder.

    Prepare folders containing the reports, differentially expressed genes
    and geneset tables for each analysis.
    '''


    spec, SPEC = TASK.get_vars(infile, infile, PARAMS)

    sample = Path(infile).parts[0].split(".")[0]

    cluster_run = spec.components + "_" + spec.resolution

    out_dir = os.path.join("reports.dir",
                           ".".join([sample, cluster_run]))

    run_dir = Path(os.path.dirname(infile)).parents[0]

    try:
        shutil.rmtree(out_dir)
    except FileNotFoundError:
        pass

    os.mkdir(out_dir)

    between_xlsx = "markers.between." + \
                   str(PARAMS["findmarkers_between_testfactor"]) + \
                   ".summary.table.xlsx"

    targets = [os.path.join(run_dir,"latex.dir","geneExpressionReport.pdf"),
               os.path.join(run_dir,"latex.dir","summaryReport.pdf"),
               os.path.join(run_dir,"latex.dir","clusterMarkerReport.pdf"),
               os.path.join(run_dir,"cluster.markers.dir","markers.summary.table.xlsx"),
               os.path.join(run_dir,"condition.markers.dir",between_xlsx),
               os.path.join(run_dir, "cluster.genesets.dir","cluster.genesets.xlsx"),
               os.path.join(run_dir, "condition.genesets.dir","condition.genesets.xlsx")]

    for target_file in targets:


        if os.path.exists(target_file):

            target = os.path.basename(target_file)

            link_name = os.path.join(out_dir, target)

            os.symlink(os.path.relpath(target_file, start=out_dir),
                       link_name)

    IOTools.touch_file(outfile)


# ########################################################################### #
# ############## Generate cellbrowser output for sharing #################### #
# ########################################################################### #

@active_if(PARAMS["run_cellbrowser"])
@follows(mkdir("cellbrowser.dir"), summaryReport)
@transform("*.seurat.dir",
           regex(r"(.*).seurat.dir"),
           r"cellbrowser.dir/\1/cellbrowser.sentinel")
def cellbrowser(infile, outfile):
    '''
    Prepare cellbrowser instance for exploratory analysis or to share with
    collaborators. A cellbrowser instance is only generated for a defined
    runspecs configuration and only once per sample.'''

    # read in yml entries
    samples_specs = [s for s in PARAMS.keys()
                     if s.startswith("cellbrowser_")]
    samples_specs = [s for s in samples_specs if not "run" in s]

    # only run if sample ID from job is listed in yml
    sample_name = infile[:-len(".seurat.dir")]

    log_file = outfile.replace(".sentinel", ".log")

    outdir = os.path.dirname(outfile)
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    # make cellbrowser if sample is mentioned in yml file
    if "cellbrowser_"+sample_name in samples_specs:
        settings_use = PARAMS[str("".join([k for k in samples_specs
                                           if str(sample_name) in k]))]
        outdir_settings = os.path.join(outdir, str(settings_use))
        # set up required subfolders
        if not os.path.exists(outdir_settings):
            os.makedirs(outdir_settings)
        # cellbrowser input files written into following folder
        outdir_folder = os.path.join(outdir_settings, "infiles")
        if not os.path.exists(outdir_folder):
            os.makedirs(outdir_folder)

        seurat_path = sample_name + ".seurat.dir"

        # Rscript to generate input files

        # set the job threads and memory
        locals().update(
            TASK.get_resources(memory=PARAMS["resources_memory_standard"]))


        statement = '''Rscript %(tenx_dir)s/R/cellbrowser_prep.R
                         --outdir=%(outdir_folder)s
                         --seurat_path=%(seurat_path)s
                         --runspecs=%(settings_use)s
                       &> %(log_file)s
                      '''
        P.run(statement)

        # python code to make configuration file
        out = open(os.path.join(outdir_settings, "cellbrowser.conf"), "w")
        conf = ""
        # cannot use projectname from pipeline.yml here as only letters/digits
        # allowed in name
        conf += 'name = "seuratPipeline"\n'
        conf += '''coords = [{"file":"infiles/UMAP.tsv","shortLabel":"UMAP"},
                             {"file":"infiles/FA.tsv","shortLabel":
                              "PAGA initiated force-directed graph"}]\n'''
        conf += 'shortLabel = "%s"\n' %PARAMS["projectname"]
        conf += 'exprMatrix = "infiles/exprMatrix.tsv.gz"\n'
        conf += 'meta = "infiles/meta.tsv"\n'
        conf += 'enumFields = ["cluster"]\n'
        conf += 'clusterField = "cluster"\n'
        conf += 'labelField = "cluster"\n'
        conf += 'colors = "infiles/colors.tsv"\n'
        conf += '''markers = [{"file": "infiles/markers.tsv",
                              "shortLabel": "Cluster markers identified by Seurat"}]\n'''
        out.write(conf)
        out.close()


        # python code to run cellbrowser
        cellbrowser_log = os.path.join(outdir_settings,
                                       "build_cb.log")
        cellbrowser_conf = os.path.join(outdir_settings, "cellbrowser.conf")
        outdir_cellbrowser = os.path.join(outdir_settings, "outfiles")
        statement = '''cbBuild -i %(cellbrowser_conf)s
                                -o %(outdir_cellbrowser)s
                                &> %(cellbrowser_log)s '''
        P.run(statement)

    else:
        # no cellbrowser for this sample
        statement = ''' echo "Do not generate cellbrowser"
                        > %(log_file)s '''
        P.run(statement)


    # add README to output folder
    readme_file = "cellbrowser.dir/README"
    if not os.path.exists(readme_file):
        statement = ''' echo "# to run cellbrowser, go to the chosen sample "
                        >> %(readme_file)s ;
                        echo "# folder (e.g. wildtype/30_0.8_1_wilcox) and use the "
                        >> %(readme_file)s ;
                        echo "# following command to open it on a port of your choice: "
                        >> %(readme_file)s ;
                        echo "cbBuild -i cellbrowser.init -o outfiles/ -p 8888"
                        >> %(readme_file)s  '''
        P.run(statement)

    IOTools.touch_file(outfile)


# --------------------------- < report target > ----------------------------- #

# This is the target normally used to execute the pipeline.

@follows(export, cellbrowser)
def report():
    pass


# ########################################################################### #
# ######################### Auxillary functions ############################# #
# ########################################################################### #

@transform(cluster,
           regex(r"(.*)/cluster.sentinel"),
           r"\1/cluster_counts.rds")
def aggregateUMIsPseudobulks(infile, outfile):
    '''
    Aggregate UMI counts across cells within cluster to form pseudobulks.

    Useful for performing e.g. DESeq2 analysis of clusters from
    multiple samples.
    '''

    outdir = os.path.dirname(infile)
    cluster_ids = os.path.join(outdir, "cluster_ids.rds")

    seurat_dir = Path(outfile).parents[1]
    sample_data_dir = str(seurat_dir).replace(".seurat", "")
    run_dir = Path(seurat_dir).parents[0]

    tenxdir = os.path.join(run_dir, 'data.dir', sample_data_dir)

    log_file = os.path.join(outdir, 'aggregated_clusters.log')

    locals().update(
        TASK.get_resources(memory=PARAMS["resources_memory_low"]))

    statement = '''Rscript %(tenx_dir)s/R/aggregate_umis_pseudobulks.R
                           --tenxdir=%(tenxdir)s
                           --clusterids=%(cluster_ids)s
                           --outfile=%(outfile)s
                           &> %(log_file)s
                        '''

    P.run(statement)


# ------------------------ < auxillary target > ----------------------------- #

@follows(aggregateUMIsPseudobulks)
def aux():
    pass


# ########################################################################### #
# ##################### full target: to run all tasks ####################### #
# ########################################################################### #

@follows(report)
def full():
    pass


# ------------------- < ***** end of pipeline **** > ------------------------ #

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
